[{"id":0,"href":"/docs/section-1/subsection-1/index.html","title":"1.1 Overview of Transformer Architecture","section":" Foundations of Generative Inference","content":" Overview of Transformer Architecture # The transformer architecture has become the foundational backbone of modern generative AI, unifying models across language, vision, and multimodal domains. At its core, a transformer replaces the sequential computation of earlier neural network architectures with parallel attention mechanisms that allow every token to directly interact with every other token. This design not only captures long-range dependencies with ease, but also scales efficiently with larger models and datasets, enabling today’s breakthroughs in large language models (LLMs), diffusion models, vision-language systems, etc. In this section, we outline the key components of the transformer—its attention layers, feed-forward sublayers, normalization and residual pathways—and describe how they come together to form deep, scalable networks optimized for both training and inference.\nThe transformer architecture (Vaswani et. al., 2017) The transformer architecture was proposed in the paper Attention is All You Need, 2017. Traditionally, it consists of an encoding component, a decoding component, and connections between them. The encoding and decoding components are comprised of a stack of encoders and decoders, respectively \u0026ndash; each of which has an identical architecture (albeit not the same weights). Historically, the wide variety of language models are broadly categorized based on the presence of encoding and decoding components as follows:\nEncoder-only models like BERT and RoBERTa, consist only of the encoder component. They process inputs bidirectionally, and the output of encoder-only models is a sequence of contextualized embeddings, making them ideal for discrimative tasks like classification, retrieval, and token-level tagging. Typical sizes of these models ranged from 300M to 1B parameters. These models rarely exceeded a few billion parameters because their primary use cases do not benefit significantly from extreme scale.\nEncoder-Decoder models refer to the original transformer architecture proposed by Vaswani et al., in which both encoding and decoding components are present. Models like T5 and BART were designed to understand an input sequence and then generate a related output. They were meant for conditional generation tasks such as machine translation, summarization and instruction following. Typical model sizes ranged from 60M to 11B parameters, and training larger models became prohibitively expensive because both encoder and decoder must scale together.\nDecoder-only models encompass the most popular and prominent architectures of models like GPT-3/4/5, Llama, Mistral, and Claude, used in chatbots and assistants. Their primary purpose is to generate text autoregressively, predicting the next token conditioned on all previous ones, making them well suited for open-ended generation. Modern decoder-only LLMs can perform nearly all tasks that encoder-only and encoder-decoder models were originally designed for, i.e., classification, retrieval, translation, summarization, question-answering, reasoning, etc. The primary reason behind this lies in the scale, i.e., the sizes of the models and the datasets on which they were trained (typically, they can go all the way up to 2T parameters).\nDecoder-only models for autoregressive text generation Compared to encoder-decoder and encoder-only models, decoder-only architectures are much simpler to scale up in size using a variety of optimization techniques (parallelization, KV caching, etc.). As a result, even though they use a causal mask for autoregressive next-token prediction, they demonstrate an implicit understanding of tasks that traditionally required bidirectional understanding. In other words, even tasks that seem discriminative (classification, retrieval, etc.) can be reframed as Generate the label, Extract the relevant phrase, Answer the question, etc.\nEach transformer block consists of:\nSelf-Attention block Positional-encoding Feed-forward network Layer Normalization Self-Attention Mechanism # The core idea of self-attention is to calculate an output vector (aka embedding/representation) for every token in the input sequence. The token whose representation is being computed is referred to as a query, and every other token in the sequence has an associated key and a value vector. The output vector is obtained by computing a weighted sum of all values in the input sequence, where the weight assigned to each value is determined by a compatibility function between the value\u0026rsquo;s associated key and the specific query being processed. This allows the model t capture dependencies between all elements in a sequence.\nThe input to the self-attention layer consists of the out put embeddings from the previous later, which are transformed into three distinct vectors for each token $i$:\nQuery $(Q_i)$: Represents what the token is looking for. Key $(K_i)$: Represents what the token offers to the other tokens. Value $(V_i)$: The actual content information that is aggregated. The self-attention output for token $i$ is calculated by: $${\\rm Attention} (Q_i, K, V) = \\sum_{j} \\alpha_{ij}V_j,$$ where $\\alpha_{ij}$ is the attention weight from token $i$ to token $j$.\nScaled Dot-Product Attention # The most common form of self-attention is the Scaled Dot-Product Attention (SDPA). In this, first, the input sequence of embeddings, $X \\in \\mathbb{R}^{n \\times d}$ (each row corresponds to a token), where $n$ is the sequence length and $d$ is the embedding dimension, is transformed into the Query, Key, and Value matrices $(Q, K, V)$ using three learned linear projection matrices $(W^Q, W^K, W^V)$ as follows: $$Q = XW^Q, \\quad K = XW^K, \\quad \\text{and} \\hspace{2ex} V = XW^V.$$ Here, $W^Q \\in \\mathbb{R}^{d \\times d_k}$, $W^K \\in \\mathbb{R}^{d \\times d_k}$ and $W^V \\in \\mathbb{R}^{d \\times d_v}$. Consequently, the output dimensions of these linear projections are $Q \\in \\mathbb{R}^{n \\times d_k}$, $K \\in \\mathbb{R}^{n \\times d_k}$ and $V \\in \\mathbb{R}^{n \\times d_v}$. Typically, $d_k = d_v = d/h$, where $h$ is the number of attention heads (for Multi-head Attention, as we will see later in Section 2).\nLinear projections to compute Q, K and V. Every row in the X matrix corresponds to a token in the input sequence (credits: The Illustrated Transformer, Jay Alammar) The compatibility score between all queries and keys is calculated using a dot product $QK^\\top \\in \\mathbb{R}^{n \\times n}$. These scores are then scaled by dividing by $\\sqrt{d_k}$ to counteract the effect of large dot-product values, and subsequently passed through a row-wise softmax function to obtain the fnal attention weights $A \\in \\mathbb{R}^{n \\times n}$. Mathematically, $$A = {\\rm softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right).$$ Note that these weights are normalized probability distributions, ensuring each row sums to $1$. The final output matrix is a weighted sum of the Value matrix $V$, using the attention weights $A$, i.e., $${\\rm Attention}(Q, K, V) = {\\rm softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V \\in \\mathbb{R}^{n \\times d_v}$$\nScaled Dot-Product Attention (SDPA) (credits: The Illustrated Transformer, Jay Alammar) Causal Self Attention (Masked Self-Attention) # this is the most crucial vairation of self-attention used exclusively in the decoder blocks of models for autoregressive generation tasks (i.e., generating one token at a time). Causality comes from the fact that in autoregressive generation, the prediction for the current token $i$ must only depend on the preceding tokens $(1,2, \\ldots, i-1)$ and not on any subsequent tokens $(i+1, i+2, \\ldots, n)$.\nCausal self-attention is implemented by applying a mask to the $\\frac{QK^\\top}{\\sqrt{d_k}}$ matrix before the softmax step. Mathematically, $$A = {\\rm softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}} + M\\right).$$ Here, the mask $M \\in \\mathbb{R}^{n \\times n}$ is an upper -triangular matrix of $-\\inf$, i.e., $M_{ij} = 0$ for $j \\leq i$ and $M_{ij} = \\inf$ for $j \u0026gt; i$. Note that when the mask is added to the scaled scores, it sets the scores for all future tokens to $-\\inf$. Subsequently, when the softmax is applied, $e^{-\\inf} = 0$, effectively ensuring that the attention weights $A_{ij}$ for $j \u0026gt; i$ (future tokens) are zero. In other words, the output for token $i$ only aggregates information only from $V_1, V_2, \\ldots V_i$.\nMulti Head Attention (MHA) # MHA is a core optimization, in which instead of computing a single attention distribution over the input sequence, the model\u0026rsquo;s hidden dimension is split into multiple heads. Each head learns its own set of query, key, and value projection, and the model computes scaled dot-product attention independently for each head, resulting in multiple attenton outputs. These outputs are then concatenated and linearly projected back to the model\u0026rsquo;s original dimension.\nSuppose the input to the attention layer is a sequence of hidden states,\n$$X \\in \\mathbb{R}^{T \\times d_{\\text{model}}},$$\nwhere $T$ is the sequence length, and $d_{\\text{model}}$ is the embedding dimension. For each head $h \\in {1, \\ldots, H}$ (where $H$ is the number of attention heads), we compute separate query, key, and value matrices via learned linear projections:\n$$Q_h = XW_h^Q, \\hspace{3ex} K_h = XW_h^K, \\hspace{2ex} V_h = XW_h^V$$\nwhere $W_h^Q, W_h^K, W_h^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$. Here, $d_k = d_{\\text{model}}/H$ is the dimension per head.\nEach head performs scaled dot-product attention:\n$$O_h \\triangleq \\text{Attention}(Q_h, K_h, V_h) = \\text{softmax}\\left(\\frac{Q_hK_h^\\top}{\\sqrt{d_k}}\\right)V_h \\in \\mathbb{R}^{T \\times d_k}.$$\nThe outputs from all heads are concatenated to get:\n$$O = \\text{Concat}(O_1, O_2, \\ldots, O_H) \\in \\mathbb{R}^{T \\times d_{\\text{model}}}$$\nA final linear projection matrix $W^O \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{model}}}$ yields the final output:\n$$\\text{MHA}(X) = OW^O$$\nMulti-Head Attention (MHA) Positional Encoding # Positional encodings are a method used to inject information about the relative or absolute position of tokens into the input sequence, enabling the model to utilize token order. Modern LLMs like the (Llama family of models) encode relative positional information directly into the attention mechanism, using methods like Rotary Positional Embedding (RoPE).\nWhile the original transformers paper used absolute positional information in the encoding, the primary goal of RoPE is to endow the self-attention mechanism with the desriable property of length generalization, meaning the model\u0026rsquo;s ability to handle sequences longer than those seen during training.\nRoPE works by applying a rotation matrix $R_{\\theta,m}$ to the $Q$ and $K$ vectors based on their token index $m$, i.e., $$Q_m^\\prime = R_{\\theta,m}Q_m \\quad \\text{and} \\quad K_n^\\prime = R_{\\theta,n}K_n,$$ where $m$ and $n$ are the positions of the current query and key, respectively. The rotation matrix is given by,\nRotary Positional Embeddings (RoPE) The rotation is performed pair-wise on the dimensions of the embedding vector (RoFormer, 2021) as shown above, and the frequencies $\\theta$ for rotation are fixed, pre-defined values.\nThe magic of RoPE lies in how it affects the dot product. The goal is to make the attention score (i.e., the dot product $Q_m^\\prime (K_n^\\prime)^\\top$) depend only on the relative distance, $m-n$. In other words, the RoPE encodings satisfy: $$\\left\\langle R_{\\theta,m}Q_m, R_{\\theta,n}K_n \\right\\rangle = \\left\\langle R_{\\theta,m-n}Q_m, K_n \\right\\rangle.$$ This mathematically enforces the desried relative position dependency directly into the attention mechanism. RoPE is particularly beneficial for casual self-attention, as it naturally handles the increasing relative distance when the sequence grows as more and more tokens are generated autoregressively. Note that RoPE is an in-place transformation that directly integrates relative positional information into the attention score computation (unlike traditional methods that add a positional vector to the input token embeddings before the first attention layer).\nInference optimization for RoPE: Since the rotation matrix calculation involves trigonometric functions, it can be computationally heavy if calculated repeatedly. A common optimization to speed up training/inference is to pre-calculate and cache the ${\\rm sin}(m\\theta_i)$ and ${\\rm cos}(m\\theta_i)$ vectors for all positions $m$ up to the maximum length. Then, rotation is simply a element-wise multiplication and addition using the pre-cached values, avoiding redundant trigonometric function calls.\nMoreoever, since computing RoPE encodings involves specific element-wise operations like slicing the $Q$/$K$ vector, rotating and putting it back, it can be further sped up using fused kernels that combines these sequential operations, reducing the number of memory accesses between the compute cores and HBM.\nFeed-Forward Network (FFN) # The feed-forward network (FFN) is the second major sublayer in every transformer block, following the self-attention mechanism. It is responsible for transforming the attention-refined featres independently for each poisition in the sequence, allowing the model to learn complex non-linear relatonships. The FFN acts position-wise, meaning the exact same. FFN is applied to every single token\u0026rsquo;s embedding vector independently and identically. It does not mix information across different position, unlike the self-attention layer. The FFN typically consists of two linear transformation with a non-linear activation in between:\nExpansion layer (up-projection): Increases the dimension of the embedding Activation function: Introduces non-linearity Contraction layer (down-projection): Returns the dimension to the original size. While the original transformer used ReLU as the activation function many modern LLMs like Llama have adopted a more sophisticated gating mechanism within the FFN, most commonly using the SwiGLU activation function. SwiGLU uses three matrices: two for the expansion phase, and one for the contraction phase. For an input $x$, it is mathematically defined as: $${\\rm SwiGLU}(x, W_{up}, W_{gate}, W_{down}) = ((xW_{up}) \\odot ({\\rm SiLU}(xW_{gate})))W_{down},$$ where $W_{up}, W_{gate} \\in \\mathbb{R}^{d \\times d_{\\rm ff}}$, $W_{down} \\in \\mathbb{R}^{d_{\\rm ff} \\times d}$, $\\odot$ denotes the element-wise Hadamard product, and $\\rm SiLU$ is the sigmoid linear unit activation function.\nThe gated path via $\\rm SiLU$ acts as a dynamic filter, determining which information from the linear path, i.e., $xW_{up}$ is allowed to pass through to the next layer and how strongly. Typically, $d_{\\rm ff}$ four times the model\u0026rsquo;s embedding dimension $d$. The $\\rm SiLU$ function is a smooth, non-monotonic activation often used in place of ReLU, and is defined as: $${\\rm SiLU}(z) = z\\cdot \\sigma(z),$$ where $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the standard sigmoid function.\nInference optimization for FFN: The FFN layer receives output from the attention sublayer. In the complete transformer block, the FFN is integrated using a residual connection and a Layernorm step, i.e., $$Y = {\\rm LayerNorm}(X_{\\rm attn} + {\\rm FFN}(X_{\\rm attn}))$$. The FFN layer is a significant computatioanly intensive part of the transformer block during inference, due to the large matrix multiplications it performs. Due to their large memory footprint, quantizing the weights of the FFN layers to low-precision formats like FP8 is pretty common. Moreover, since the SwiGLU calculation involves several sequential element-wise operations, they can be fused tohether into a single, unified kernel, reducing launch overheads. More recently, the FFN layer has been replaced by Mixture-of-Experts (MoE) layers, which we will look in details later.\nLayer Normalization (LayerNorm) # In general, LayerNorm is a technique used to normalize the activations. of the neurons across the features (hidden dimensions). For a given input vector $x = (x_1, \\ldots, x_d)$ from a layer\u0026rsquo;s output, LayerNorm calculates the normalized output $y$ as follows:\nFirst, the mean $\\mu$ and variance $\\sigma^2$ are calculated across all $d$ features as $\\mu = \\frac{1}{d}\\sum_{i=1}^{d}x_i$ and $\\sigma^2 = \\frac{1}{d}\\sum_{i=1}^{d}(x_i - \\mu)^2$. Next, each element $x_i$ is normalized using the calculated mean and variance as $\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$, where $\\epsilon$ is a small constant (like $10^{-5}$) added for numerical stability to prevent division by zero in case the variance is zero. The output $\\hat{x}_i$ now has a mean of zero and a variance of one across the hidden dimension. The normalized output $\\hat{x}_i$ is then scaled by a learnable parameter $\\gamma$ and shifted by another learnable parameter $\\beta$. These two parameters allow the model to rescale the normalized values as $y_i = \\gamma_i \\hat{x}_i + \\beta_i$, providing a unique scale $(\\gamma_i)$ and shift $(\\beta_i)$ factor for each feature dimension $i$. The scale $(\\gamma)$ and shift $(\\beta)$ parameters are vectors of size $d$, and are learned during the training process via backpropagation. In the standard transformer layer, LayerNorm is typically placed in a Post-Norm configuration, i.e., as part of the structure: $$H = {\\rm LayerNorm}(X + {\\rm SubLayer}(X)),$$ where $X$ is the input to the SubLayer (either Self-Attention of Feed-Forward Network). The final output $H$ is the result of applying LayerNorm to the sum of the input $X$ and the sublayer\u0026rsquo;s output (also known as Residual Connection).\nInference optimization for LayerNorm: Layernorm poses a bottleneck during inference primarily for two reasons:\nSince it is a row-wise operation, meaning it calculates the mean and variance idnependently for each row/token in the sequence, the input data must be read from the main memory, processed, and written back. Unlike large matrix multiplication (like $QK^\\top$), which reuse the same input data multiple time (i.e., high arithmetic intensity), LayerNorm has very little data reuse, making it a memory-bandwidth bound operation. Moreover, the normalization formula involves calculating the mean, variance, square root, and division. The mathematical operations are complex to implement efficiently on hardware, particularly when using standard, high-precision floating-point arithmetic. The most common optimization to overcome this bottleneck is to use kernel fusion, i.e., the sequence of mathematical operationsthat constitue Layernorm are fused into a single kernel, drastically reducing the need to repeatedly read intermediate results from memory and write them back, improving compute core utilization. Furthermore, computation of non-linear functions are sped up using approximation techniques like look-up tables (LUTs), piecewise-linear approximations, etc.\nOverview of Inference Computations # Prefill and Decoding stages in Autoregressive decoding Autoregressive inference for LLMs is the process of generating an output continuation ($Y$) from an input prompt ($X$), and is fundamentally divided into two distinct computational phases: the Prefill Stage and the Decoding Stage. Understanding this division is essential because the computational and memory bottlenecks differ significantly between the two.\nPrefill Stage (Prompt Processing) This stage processes the entire input sequence $X$ to prepare the model for generation.\nInput: The entire prompt sequence $X = {x_1, x_2, \\dots, x_L}$. Computation: The model performs a full forward pass, calculating the self-attention for all $L$ tokens simultaneously. Computational Cost: The primary cost is in the self-attention mechanism, which scales quadratically with the length of the input sequence $L$, denoted as $O(L^2)$. This is the most computationally expensive part of the entire inference process for long prompts. Output: The model generates the probability distribution for the first output token, $y_1$. Crucially, it also computes and stores the Key ($K$) and Value ($V$) vectors for every input token in a memory buffer called the KV-Cache. Prefill processes the entire prompt, generates the first token, and populates the KV cache Decoding Stage (Token Generation) After the first token is generated at the end of prefill, this stage iteratively generates the output sequence $Y$ one token at a time.\nInput: The last generated token ($y_{t-1}$) and the previously computed $K$ and $V$ vectors (the KV-Cache). Computation: The model performs a single-step forward pass (one token). Since the $K$ and $V$ vectors for all prior tokens are already cached, the attention calculation only needs to consider the new token against the existing KV-Cache. Computational Cost: This stage is memory-bound, as the model spends most of its time fetching the growing KV-Cache from memory rather than performing complex arithmetic. Output: A new token $y_t$ is generated, and its corresponding $K$ and $V$ vectors are appended to the KV-Cache. This step repeats until $n$ tokens are generated or a stopping condition is met. Autoregressive decoding generates the subsequent tokens, one-at-a-time and keeps appending to the KV cache To optimize the prefill stage, focus is generally on speeding up matrix multiplication and attention calculations (e.g., FlashAttention). On the other hand, optimizing decoding stage focuses on managing the large KV-cache memory footprint (e.g., PagedAttention and GQA).\nReferences # Vaswani et al., 2017, Attention is All you Need https://arxiv.org/abs/1706.03762 The Illustrated Transformer, Jay Alammar https://jalammar.github.io/illustrated-transformer/ Raffel et al., 2019, Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer https://arxiv.org/abs/1910.10683 Sebastian Raschka, Blogpost: Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch, 2023 https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html Lewis et al., 2019, BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension https://arxiv.org/abs/1910.13461 Devlin et al., 2018, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://arxiv.org/abs/1810.04805 Liu et al., 2019, RoBERTa: A Robustly Optimized BERT Pretraining Approach, https://arxiv.org/abs/1907.11692 Su et. al., RoFormer: Enhanced Transformer with Rotary Position Embedding, 2021 https://arxiv.org/abs/2104.09864 LayerNorm, NKI API Docs, https://awsdocs-neuron.readthedocs-hosted.com/en/latest/nki/tutorials/layernorm.html "},{"id":1,"href":"/docs/section-2/subsection-1/index.html","title":"2.1 Efficient Attention Variants","section":"Algorithmic and Modeling-level Inference Optimization","content":" 2.1 Efficient Attention Variants # At the heart of the success of transformer architectures, lies the attention mechanism, which allows models to capture long-range dependencies and complex interactions across inputs. However, standard self-atention scales quadratically with sequence length, making it prohibitively expensive for large-scale or realtime applications. This has spurred a wave of research into efficient attention mechanisms \u0026ndash; techniques that reduce the computational and memory overhead without sacrificing accuracy. Some of them are highlighted here.\nMulti Head Attention (MHA) # MHA is a core optimization, in which instead of computing a single attention distribution over the input sequence, the model\u0026rsquo;s hidden dimension is split into multiple heads. Each head learns its own set of query, key, and value projection, and the model computes scaled dot-product attention independently for each head, resulting in multiple attenton outputs. These outputs are then concatenated and linearly projected back to the model\u0026rsquo;s original dimension.\nSuppose the input to the attention layer is a sequence of hidden states,\n$$X \\in \\mathbb{R}^{T \\times d_{\\text{model}}},$$\nwhere $T$ is the sequence length, and $d_{\\text{model}}$ is the embedding dimension. For each head $h \\in {1, \\ldots, H}$ (where $H$ is the number of attention heads), we compute separate query, key, and value matrices via learned linear projections:\n$$Q_h = XW_h^Q, \\hspace{3ex} K_h = XW_h^K, \\hspace{2ex} V_h = XW_h^V$$\nwhere $W_h^Q, W_h^K, W_h^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$. Here, $d_k = d_{\\text{model}}/H$ is the dimension per head.\nEach head performs scaled dot-product attention:\n$$O_h \\triangleq \\text{Attention}(Q_h, K_h, V_h) = \\text{softmax}\\left(\\frac{Q_hK_h^\\top}{\\sqrt{d_k}}\\right)V_h \\in \\mathbb{R}^{T \\times d_k}.$$\nThe outputs from all heads are concatenated to get:\n$$O = \\text{Concat}(O_1, O_2, \\ldots, O_H) \\in \\mathbb{R}^{T \\times d_{\\text{model}}}$$\nA final linear projection matrix $W^O \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{model}}}$ yields the final output:\n$$\\text{MHA}(X) = OW^O$$\nThis parallel attention structure has two key benefits:\nIt increases the models\u0026rsquo; representation capacity without increasing computational complexity as much as a single very large attention would, It allows the model to capture richer contextual information because different heads can specialize in attenting to different positions or patterns in the sequence. For example, one head might learn to track subject-verb agreement, while another focuses on long-range dependencies. The final output provides a more expressive representation that downstream layers can use effectively.\nGrouped Query Attention (GQA) # Where standard MHA has one set of key/value projections per query head, GPQ reduces the number of key/value heads to save memory and bandwidth during inference, while keeping the same number of query heads for expressivity. Let $G$ be the total number of key/value heads (with $G \u0026lt; H$, i.e., the number of query heads). Denote by $d_q = d_{\\text{model}}/ H$ and $d_{kv} = d_{\\text{model}}/G$ be the dimension per query head and per KV head, respectively.\nFor each head $h = 1, \\ldots, H$, the query is given by $Q_h = XW_h^Q$, where $W_h^Q \\in \\mathbb{R}^{d_{\\text{model}} \\times d_q}$.\nFor keys and values (fewer heads), for $g = 1, \\ldots G$ and $W_g^K, W_g^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{kv}}$, we have,\n$$K_g = XW_g^K \\hspace{2ex} \\text{and} \\hspace{2ex} V_g = XW_g^V.$$\nEach query head $h$ is assigned to one key/value head $g$. For example, if $r = H/G$ (an integer), then $g = \\left\\lfloor \\frac{h}{r} \\right\\rfloor$. This means $r$ query heads share the same key/value pair. Subsequently, for each query head $h$, we attend to the key/value head $g$ it\u0026rsquo;s mapped to, to get:\n$$O_h \\triangleq \\text{Attention}(Q_h, K_g, V_g) = \\text{softmax}\\left(\\frac{Q_hK_g^\\top}{\\sqrt{d_q}}\\right)V_g \\in \\mathbb{R}^{T \\times d_q}.$$\nAs in standard MHA,\n$$\\text{GQA}(X) = OW^O, \\hspace{2ex} W^O \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{model}}}, \\hspace{2ex} O = \\text{Concat}(O_1, O_2, \\ldots, O_H) \\in \\mathbb{R}^{T \\times d_{\\text{model}}}.$$\nWith GQA, the KV cache size is given by,\n$$\\text{batchsize } * \\text{ seqlen } * \\text{ (embed-dim / nheads) } * \\text{ nlayers } * \\hspace{1px} 2 \\hspace{1px} * \\text{ precision } * \\text{ nKVheads }$$\nGQA offers a balance between efficiency and expressivity in transformer architectures. By allowing many query heads to share a smaller number of KV heads, GQA drastically reduce the memory footprint and bandwidth cost of KV caching during autoregressive inference \u0026ndash; often by a factor of $H/G$, where $H$ is the number of query heads and $G$ is the number of KV heads. This reduction leads to faster decoding, lower latency, and better scalability to longer context lengths. In practice, this enables deploying larger or more capable models under the same memory budget, making GQA a key ingredient in efficient long-context language models like Llama 2 and Llama 3.\nSliding Window Attention (SWA) # In standard full attention, each token attend to every other token in the sequence. This results in quadratic complexity, i.e., $\\mathrm{O}(N^2)$, where $N$ is the sequence length. SLiding window Attention (SWA) changes this by restricting the attention span. In SWA, each token only attends to tokens within a fixed window of size $w$ around it (e.g., for casual SWA, this corresponds to the previous $w$ tokens). This reduces the computational complexity to $\\mathrm{O}(Nw)$, instead of $\\mathrm{O}(N^2)$, which is particularly useful for long contexts such as long documents or streaming scenarios, without blowing up GPU memory. With SWA, the KV cache size is given by,\n$$\\text{batchsize } * \\text{ W } * \\text{ (embed-dim / nheads) } * \\text{ nlayers } * \\hspace{1px} 2 \\hspace{1px} * \\text{ precision } * \\text{ nKVheads. }$$\nThat is, SWA reduces the KV cache size by a factor of $\\text{W / seqlen}$\nComparison of standard attention vs sliding window attention (credits: Sebastian Raschka) References # Sliding Window Attention (SWA) https://sebastianraschka.com/llms-from-scratch/ch04/06_swa/ "},{"id":2,"href":"/docs/section-3/subsection-1/index.html","title":"3.1 Paged Attention","section":"Systems-Level Inference Optimization","content":" 3.1 Paged Attention # "},{"id":3,"href":"/docs/section-4/subsection-1/index.html","title":"Section 4.1","section":"Open-Source Tools, Frameworks, and Deployment Scenarios","content":" Section 4.1 # Welcome! Content for Section 4.1\n"},{"id":4,"href":"/docs/section-1/subsection-2/index.html","title":"1.2 Llama family of models","section":" Foundations of Generative Inference","content":" Llama family of models # The Llama family, including Llama (Feb 2023), Llama-2 (Jul 2023), Llama-3, 3.1, 3.2, 3.3 (Apr 2024 - Dec 2024), and Llama-4 (Apr 2025), are open-source, decoder-only transformer models developed by Meta. Their design choices significantly influenced the efficiency landscape of LLMs, making them highly relevant for discussing optimization.\nThe architecture introduced several key modifications specifically aimed at improving training stability and, most importantly, inference speed and memory footprint. For example,\nRMSNorm (Root Mean Square Normalization): Llama uses RMSNorm instead of the standard LayerNorm (used in the original transformer). This variant removes the mean centering step and only scales the vector by its root mean square (RMS), i.e., $${\\rm RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d}\\sum_{i=1}^{d}x_i^2 + \\epsilon}}\\gamma$$ Removing the mean calculation and bias addition in traditional LayerNorm, simplifies the overall operation and improves efficiency and speed during both training and ifnerence, often with negligible difference in final model quality.\nRotary Positional Embeddings (RoPE): RoPE encodes positional information by rotating the query and key vectors in each attention layer. It was desgined to enable the model to better extrapolate to longer context lengths \u0026ndash; a crucial factor for serving long-running chat sessions or processing large documents.\nGrouped Query Attention (GQA): Introduced in larger versions on Llama2 and adopted across the Llama3 family, GQA is perhaps the most significant inference optimization. Instead of having a separate key and value projection for every query head (as in MHA), GQA divides the query heads into groups that share a single K and V projection. The primary benefit is a drastic reduction in the size of the KV-Cache (the stored K and V vectors from previous tokens). This is critical because the KV-Cache is a major bottleneck for LLM memory consumption, scaling linearly with batch size, sequence length, and the number of layers. GQA allows for significantly faster sequential decoding (generating the next token) and higher throughput (more concurrent requests).\nMixture-of-Experts (MoE): Llama-4 transitions from a dense transformer to a Mixture-of-Experts (MoE) architecture. Instead of applying the same feed-forward block to every token, a learned router selects just a few specialized experts. This allows the model to scale to much larger parameter counts without increasing compute or latency proportionally.\nThe following is a snapshot of config.json for Llama 3.1 8b from HuggingFace:\n{ \u0026#34;architectures\u0026#34;: [ \u0026#34;LlamaForCausalLM\u0026#34; ], \u0026#34;attention_bias\u0026#34;: false, \u0026#34;attention_dropout\u0026#34;: 0.0, \u0026#34;bos_token_id\u0026#34;: 128000, \u0026#34;eos_token_id\u0026#34;: 128001, \u0026#34;hidden_act\u0026#34;: \u0026#34;silu\u0026#34;, \u0026#34;hidden_size\u0026#34;: 4096, \u0026#34;initializer_range\u0026#34;: 0.02, \u0026#34;intermediate_size\u0026#34;: 14336, \u0026#34;max_position_embeddings\u0026#34;: 131072, \u0026#34;mlp_bias\u0026#34;: false, \u0026#34;model_type\u0026#34;: \u0026#34;llama\u0026#34;, \u0026#34;num_attention_heads\u0026#34;: 32, \u0026#34;num_hidden_layers\u0026#34;: 32, \u0026#34;num_key_value_heads\u0026#34;: 8, \u0026#34;pretraining_tp\u0026#34;: 1, \u0026#34;rms_norm_eps\u0026#34;: 1e-05, \u0026#34;rope_scaling\u0026#34;: { \u0026#34;factor\u0026#34;: 8.0, \u0026#34;low_freq_factor\u0026#34;: 1.0, \u0026#34;high_freq_factor\u0026#34;: 4.0, \u0026#34;original_max_position_embeddings\u0026#34;: 8192, \u0026#34;rope_type\u0026#34;: \u0026#34;llama3\u0026#34; }, \u0026#34;rope_theta\u0026#34;: 500000.0, \u0026#34;tie_word_embeddings\u0026#34;: false, \u0026#34;torch_dtype\u0026#34;: \u0026#34;bfloat16\u0026#34;, \u0026#34;transformers_version\u0026#34;: \u0026#34;4.43.0.dev0\u0026#34;, \u0026#34;use_cache\u0026#34;: true, \u0026#34;vocab_size\u0026#34;: 128256 } Calculating model size # While the total parameter count (e.g., \u0026ldquo;8 billion parameters\u0026rdquo;) gives us a high-level idea of a model\u0026rsquo;s size, understanding where that memory is allocated within the architecture is crucial for true optimization. The total memory footprint of an LLM is the sum of the weights from every layer: the self-attention mechanism, the feed-forward network, and the embedding tables.\nTransformer layers of the dense Llama architecture (credits: Yuan et. al., 2024) In this section, we will use the Llama 3 8B model as our case study to perform a bottom-up calculation. We will systematically determine the size of the three primary components—the Embedding Layer, the Self-Attention Blocks, and the Feed-Forward Networks (FFN), and then sum these components to arrive at the total parameter count, demonstrating how a model\u0026rsquo;s architecture directly dictates its memory needs. This exercise will clarify the following:\nThe relationship between the hidden dimension ($d_{\\text{model}}$) and the overall model size. The overwhelming contribution of the FFN to the total parameter count. Based on the config.json for Llama 3.1 8B, the following parameters are critical for calculating the total weights (parameters):\nParameter Symbol Params hidden_size $d$ 4096 intermediate_size $d_{\\rm ff}$ 14,336 num_attention_heads $H$ 32 num_hidden_layers $N_{\\rm layers}$ 32 num_key_value_heads $G$ 8 vocab_size $V$ 128,256 The total number of parameters ($P_{\\text{total}}$) is the sum of the parameters in the three main components: the Embedding Layer, the total Attention Blocks, and the total Feed-Forward Networks (FFN), i.e., $$P_{\\text{total}} = P_{\\text{embedding}} + P_{\\text{LMhead}} + N_{\\text{layers}} \\times (P_{\\text{attention}} + P_{\\text{FFN}})$$\nEmbedding layer and LM head $(P_{\\text{embedding}} \\text{ and } P_{\\text{LMhead}})$: The embedding layer converts the input token IDs into vectors of dimension $d_{\\text{model}}$, i.e., $$P_{\\text{embedding}} = V * d = 128,256*4096 = 525,336,576 = P_{\\text{LMhead}}$$\nSelf-Attention block $(P_{\\text{attention}})$: The attention block contains the linear projection matrices for Query ($W_Q$), Key ($W_K$), Value ($W_V$), and the final Output ($W_O$) projection. Llama 3 uses Grouped-Query Attention (GQA), which changes the size of $W_K$ and $W_V$ compared to standard Multi-Head Attention (MHA).\nQuery ($W_Q$): Each of the $H$ query heads requires a matrix of size $d \\times (d/H)$, and there are $H$ of them. Total size: $d \\times d$. Key/Value ($W_K, W_V$): Each of the $G$ key/value heads requires a matrix of size $d \\times (d/H)$, and there are $2G$ of them. The effective dimension is $d \\times (G \\times d_{\\text{head}})$, where $d_{\\text{head}} = d/H$. Output ($W_O$): Projects the concatenated attention output back to $d$ and has shape $d \\times d$. So, $$P_{\\text{attention}} = d^2 \\left(1 + \\frac{2G}{H} + 1\\right) = 41,943,040.$$ Feed-forward network $(P_{\\text{FFN}})$: The FFN uses SwiGLU activation, which involves three linear layers, $W_{gate}, W_{up} \\in \\mathbb{R}^{d \\times d_{\\rm ff}}$, and $W_{down} \\in \\mathbb{R}^{d_{\\rm ff} \\times d}$. Note that the total parameters from the FFN are usually the largest component in a Transformer block, and is given by $$P_{\\text{FFN}} = 3\\cdot d \\cdot d_{\\rm ff} = 176,160,768.$$\nSo, the total number of parameters adds up to $$P_{\\text{total}} = 525,336,576 + 32 * (41,943,040 + 176,160,768) \\approx 8\\text{B}.$$\nTakeaway \u0026ndash; Informing Inference Optimization # This component-wise calculation reveals the true distribution of memory within the LLM\u0026rsquo;s architecture, which directly informs our optimization strategy:\nPrioritizing FFN Optimization: Our calculation shows that the Feed-Forward Networks (FFN) are the dominant memory and compute consumer in the model (contributing roughly 4 times the parameters of the attention block per layer). Consequently, optimization techniques that target the FFN—such as highly efficient quantization schemes, yield the most significant overall memory savings and corresponding latency reductions. As a matter of fact, more recent models like Llama-4 and GPT-OSS replace the FFN layer with MoE layers, and the weights are natively quantized to FP8 and MXFP4 formats, respectively, using Quantization-Aware training.\nAttention vs. Compute Trade-off: While the FFN determines the bulk of the weight memory, the Attention Block (especially its output, the KV-Cache) dictates the activation memory required during generation. Techniques like Grouped-Query Attention (GQA) and PagedAttention are critical for managing the attention component, particularly for long sequence lengths or large batch sizes, as their primary goal is to minimize the KV-Cache footprint, which is the key bottleneck for throughput.\nIn short, understanding this architectural breakdown allows us to strategically apply memory-saving techniques where they will have the maximum impact: focusing on FFN for static model size reduction (quantization) and on the attention mechanism for dynamic generation speed and throughput (KV-Cache management).\nReferences # Yuan et. al., LLM Inference Unveiled: Survey and Roofline Model Insights, 2024 https://arxiv.org/pdf/2402.16363\n"},{"id":5,"href":"/docs/section-2/subsection-2/index.html","title":"2.2 Mixture of Experts","section":"Algorithmic and Modeling-level Inference Optimization","content":" 2.2 Mixture of Experts # Scaling Laws for modern LLMs dictate that increasing the model size benefits generalization. A dense model with $N$ parameters executes all parameters per forward pass \u0026ndash; making inference linearly more expensive as models scale in size.\nThe Mixture-of-Experts (MoE) architecture decoduples model size from compute by activating only a small, routed subset of parameters per token. A model might store $1\\text{T}$ parameters, yet compute on only $10 - 20\\text{B}$ per token. This provides:\nHigher parameter count $\\implies$ Better quality of output Lower per-token FLOPs $\\implies$ Cheaper inference Better scaling with parallel hadrware (experts can be sharded across devices) This trade-off makes MoEs fundamentally appealing for high-throughput inference.\nMoE architecture overview # An MoE layer replaces the standard feedforward network (FFN) block in a transformer. Instead of one FFN layer, we have $E$ experts, each of which is an independent MLP. This is demonstrated below:\nMoE layers replace standard FFN layers. MHA modules are remain unchanged (credits: Lepikhin et. al.) In the figure above, every alternate transformer block has an MoE layer.\nRouting # For each input token representation $h \\in \\mathbb{R}^d$, a lightweight gating function first computes the scores:\n$$g = W_gh, \\quad \\text{where} \\quad W_g \\in \\mathbb{R}^{E \\times d}.$$\nNext, it selects the top-$k$ experts (typically $k = 1$ of $k = 2$), and routes the token\u0026rsquo;s hidden state to those experts. Formally,\n$$G(h) = \\text{Softmax}\\left(\\text{TopK}(g,k)\\right).$$\nHere, $\\text{TopK}(g,k) = g_i, \\text{if } g_i \\text{ is in the top-k elements}, -\\infty \\text{ otherwise}$. Subsequently,\n$$\\text{MoE}(h) = \\sum_{i = 1}^nG(h)_i E_i(h), \\text{ where } E_i(h) \\text{ is the output of expert } i.$$\nNote that each selected expert proceses on the tokens routed to it. Experts operate in parallel across acelerator shards. The outputs are combined using the gating weights. The above computation is visualized below:\nMoE layer computation (credits: Fedus et. al.) Why MoE improves inference efficiency # A transformer FFN (dense or expert MLP) typically consists of three linear layers: Gate projection $W_{\\text{gate}} \\in \\mathbb{R}^{d \\times f}$, Up projection $(W_{\\text{up}} \\in \\mathbb{R}^{d \\times f})$ (sometimes fused with the gate matrix), and Down projection $(W_{\\text{down}} \\in \\mathbb{R}^{f \\times d})$/. These projections expand the hidden dimension from $d$ to a larger dimension $f$, apply a non-linear activation (e.g., SiLU or GELU), then contract back. The expansion ratio $(r)$, i.e., $f/d$ refers to how much the hidden dimension expands inside the FFN relative to the model dimension.\nSince the FFN cost is dominated by the matmuls $d \\times f$ (gate, up) and $f \\times d$ (down), the compute FLOPs scales as $\\mathrm{O}\\left(3rd^2\\right)$. For an MoE with $E$ experts, top-$k$ routing has $\\mathrm{O}\\left(3krd^2\\right)$ FLOPs per expert. Since we invoke a fixed number of experts ($k$ per token \u0026ndash; for e.g., $k = 1$), the compute per token stays constant, even as $E$ increases, allowing models size to scale without scalig compute. In other words, the number of activated parameters during inferences tays fixed, even though the total model size can scale up.\nMemory trade-off: Since al lexperts\u0026rsquo; weights need to be stored, this increases VRAM usage as expected due to increasing model size.\nMoE inference optimization # Parallelism # MOE layers introduce a unique structure: a large bank of independent FFN experts \u0026ndash; only a few of which activate per token, and hence they introduce a new dimension of parallelism. The two main strategies to parallelize MoE computation are \u0026ndash; tensor parallelism and expert parallelism. Understanding when each is appropriate is essential for building efficient MoE inference systems.\nTensor parallelism (TP) splits the weights matrices inside an expert across multiple devices. Each device holds a slice of the weight matrices, and all devices jointly compute the expert\u0026rsquo;s output. TP enables very large experts, i.e., when $d$ and $f$ are large, and larger GEMMs are spread across multiple devices, enabling higher tensor-core utiliziation. However, TP also requires collective communication (all-reduce, reduce-scatter) to compute the final MoE layer output. These collectives are latency-sensitive, and if the inter-device interconnect does not have a high-enough bandwidth, these collectives can be memory-bandwidth bound, severely undermining any improvement in GEMM throughput.\nExpert parallelism (EP) partitions the experts themselves across devices, and is usually preferred when the number of experts is large. Each device holds a subset of experts, but holds all parameters for those experts. To do a forward pass through an MoE layer with EP, tokens enter the MoE layer on their originating device. Then, the gate selects expert IDs for each token, and the tokens are sent (all-to-all) to devices that own the chosen experts. Subsequently, each device computes its local experts\u0026rsquo; MLPs without further splitting, and the token outputs are returned (all-to-all) back to their original devices. routing tokens to their selected experts on other devices requires efficient inter-device communication, and is often the performance bottleneck.\nRoughly stated, EP is preferred for a large number of small experts (e.g., DeepSeek-MoE), whereas TP is preferred for a small number of large experts (e.g., Llama-4). Depending on requirements, it is also possible to combine EP + TP together in a hybrid 2D parallelism strategy. In any case, strategies to minimize the inter-device communication latency invariably helps optimize the inference latency.\nExpert-compute optimizations # While MoE does reduce the compute per token in terms of FLOPs, it introduces many small, fragmented computations that can be inefficient by default. For instance, each expert performs gate projection, up project, non-linear activation, and down projection. Naively, this yields multiple small kernels, each with separate memory loads/stores, kernel launch overhead, and poor likelihood of hitting high tensor-core occupancy. FusedMoE kernels, which ar eused by popular LLM serving frameworks like vLLM, use a single fused kernels that implements $\\text{SwiGLU}\\left(W_{\\text{up}}h, W_{\\text{gate}}h\\right)$ followed by $W_{\\text{down}}$ in one kernel invocation. This reduces the kernel launch overhead, enables intermediate activates to be stored on SRAM avoiding redundant HBM reads/writes, and improves the tensor-core utilization.\nExpert-weight quantization: Moreover, since expert weights have a high memory footprint, and loading them from HBM is often the bottleneck, several recent models have opted to quantize the expert weights to low-precision formats. For example, expert weights in GPT-OSS (from OpenAI) are quantized to MXFP4, Kimi-K2 Thinking (from MoonshotAI) has INT4 expert weights. These weights are natively quantized, and the effects of quantization error are mitigated using quantization-aware-training. The quantized weights are dequantized on-the-fly to a higher precision (such as BF16) inside fused kernels for compute.\nTo summarize, MoE architectures offer an attractive path to scaling model capacity without proportionally increasing inference cost. But achieving this theoretical efficiency in practice requires far more than sparsely activating experts: it hinges on carefully engineering the dataflow, minimizing the overhead of routing, and exploiting the right mix of expert and tensor parallelism. When these optimizations align with hardware topology and memory bandwidth constraints, MoE models can deliver dense-model quality at a fraction of the compute per token. Ultimately, MoEs exemplify the broader theme of inference optimization, i.e., performance emerges not only from algorithmic design, but from thoughtful co-design of model structure, parallelism strategy, communication patterns, and low-level kernels.\nReferences # Mixture of Experts Explained, HuggingFace blog, Dec 2023 https://huggingface.co/blog/moe Lepikhin et. al., GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding, 2020 https://arxiv.org/abs/2006.16668 Fedus, Dean and Zoph, A Review of Sparse Expert Models in Deep Learning, 2022 https://arxiv.org/abs/2209.01667 "},{"id":6,"href":"/docs/section-3/subsection-2/index.html","title":"3.2 Continuous Batching","section":"Systems-Level Inference Optimization","content":" 3.2 Continuous Batching # "},{"id":7,"href":"/docs/section-1/subsection-3/index.html","title":"1.3 Hardware Accelerators for Deep Learning","section":" Foundations of Generative Inference","content":" Hardware Accelerators for Deep Learning # "},{"id":8,"href":"/docs/section-2/subsection-3/index.html","title":"2.3 Model Quantization","section":"Algorithmic and Modeling-level Inference Optimization","content":" 2.3 Model Quantization # Quantization basics # Compressing the weights of the model via quantization has been one of the fundamental ways of reducing storage requirements for the model. Quantization can mathematically be defined as a mapping from a high precision (floating-point) representation to a lower precision representation. This process reduces the number of bits required to store and compute with each parameter, enabling models to be deployed on resource constrained devices and significantly accelerating inference.\nFormally, let $S$ be the set of source representation (e.g., FP32, FP16, BF16 and so on) and $T$ be the set of target representation (e.g., INT8, INT4, FP8 and so on), where both $S$ and $T$ are discrete sets with finite codebooks. Typically, $|S| \u0026raquo; |T|$, meaning the source representation has a significantly richer codebook that can represent more distinct values than the target representation. Quantization is then defined as a function:\n$$Q: S \\rightarrow T$$\nthat maps each value from the source representation to a value in the target representation. For a given parameter $x \\in S$, the quantized value is $x_q = Q(x) \\in T$.\nFor numerical representations, this mapping is commonly implemented using an affine transformation:\n$$x_q = Q(x) = \\text{round}(x/\\Delta) - z$$\nwhere:\n$\\Delta \\in \\mathbb{R}^+$ is the scale factor that determines the quantization step size $z \\in T$ is the zero point that shifts the quantization range (used in asymmetric quantization) $\\text{round}(\\cdot)$ maps to the nearest value in the target codebook $T$ The target representation typically uses $b$ bits per value, so $|T| = 2^b$. For instance, INT8 uses 8 bits with $T = \\{-128, -127, \u0026hellip;, 127\\}$ for signed integers. The dequantization process reconstructs an approximation in the source representation (or a compatible representation):\n$$\\hat{x} = \\Delta \\cdot (x_q + z)$$\nwhere $\\hat{x} \\approx x$ is the dequantized value. The quantization error $\\epsilon = |x - \\hat{x}|$ represents the information loss introduced by mapping from the richer codebook $S$ to the more limited codebook $T$.\nData types # Typical data type bit representations. In the context of LLM inference, floating point representations are commonly employed. Floating-point numbers follow the IEEE 754 standard and are represented using three components:\n$$x = (-1)^{\\text{sign}} \\times (1 + \\text{mantissa}) \\times 2^{(\\text{exponent} - \\text{bias})}$$\nwhere:\nSign bit (1 bit): Determines if the number is positive (0) or negative (1) Exponent bits ($e$ bits): Encodes the power of 2, stored with a bias to allow both positive and negative exponents Mantissa bits ($m$ bits): Also called the significand or fraction, represents the precision of the number Common floating-point formats include:\nFP32 (32 bits): 1 sign + 8 exponent + 23 mantissa, bias = 127, range ≈ $\\pm 3.4 \\times 10^{38}$ FP16 (16 bits): 1 sign + 5 exponent + 10 mantissa, bias = 15, range ≈ $\\pm 6.5 \\times 10^{4}$ BF16 (16 bits): 1 sign + 8 exponent + 7 mantissa, bias = 127, range ≈ $\\pm 3.4 \\times 10^{38}$ FP8 E4M3 (8 bits): 1 sign + 4 exponent + 3 mantissa, bias = 7, range ≈ $\\pm 448$ FP8 E5M2 (8 bits): 1 sign + 5 exponent + 2 mantissa, bias = 15, range ≈ $\\pm 5.7 \\times 10^{4}$ The mantissa is stored in normalized form, where the leading bit is implicitly 1 (i.e., $1.\\text{mantissa}$), except for subnormal numbers. More exponent bits provide greater dynamic range, while more mantissa bits provide greater precision.\nLet us see the floating point conversion in action; to illustrate quantization between floating-point formats, consider converting the value 6.625 from FP32 to FP8 E4M3 format.\nFP32 representation (1 sign bit, 8 exponent bits, 23 mantissa bits):\nBinary representation: $6.625_{10} = 110.101_2$ Normalized form: $1.10101_2 \\times 2^2$ Sign bit: $0$ (positive) Exponent: $2 + 127 = 129 = 10000001_2$ (bias of 127) Mantissa: $10101000000000000000000_2$ (23 bits, storing fractional part $.10101$) Full FP32: 0 10000001 10101000000000000000000 FP8 E4M3 representation (1 sign bit, 4 exponent bits, 3 mantissa bits):\nSign bit: $0$ (positive) Exponent: $2 + 7 = 9 = 1001_2$ (bias of 7 for E4M3) Mantissa: Must round $.10101$ to 3 bits → $.101_2$ (round to nearest) The 4th and 5th bits are $01$, which is less than half ($10$), so we round down Full FP8: 0 1001 101 Result: The FP8 E4M3 value represents $1.101_2 \\times 2^2 = 1.625 \\times 4 = 6.5$\nQuantization error: $\\epsilon = |6.625 - 6.5| = 0.125$\nThis example demonstrates how the limited mantissa precision in FP8 (3 bits vs. 23 bits) introduces quantization error. The representation is reduced from 32bits to 8bits, compressing the value by 4× but with precision loss.\nSources and impact of quantization error # Quantization error arises from two fundamental limitations when mapping from a richer representation $S$ to a more constrained representation $T$: reduced dynamic range and reduced precision. Understanding these error sources is critical for successful model quantization, as uncontrolled quantization error can lead to catastrophic model failure or subtle performance degradation during inference.\nReduced dynamic range: overflow and underflow # The dynamic range of a numerical format determines the span of values it can represent, from the smallest non-zero value to the largest finite value. When quantizing to a format with reduced dynamic range, values outside the representable range must be clipped, leading to overflow and underflow errors.\nOverflow occurs when $|x| \u0026gt; \\max(T)$, where the value exceeds the maximum representable value in the target format: $$Q(x) = \\begin{cases} \\max(T) \u0026amp; \\text{if } x \u0026gt; \\max(T) \\ \\min(T) \u0026amp; \\text{if } x \u0026lt; \\min(T) \\end{cases}$$\nFor example, FP8 E4M3 has a maximum value of 448, while FP32 can represent values up to $\\sim 3.4 \\times 10^{38}$. A weight value of 500 in FP32 would be clipped to 448 in FP8 E4M3, causing saturation.\nUnderflow occurs when $0 \u0026lt; |x| \u0026lt; \\min(|T|)$, where the value is smaller than the smallest representable non-zero value: $$Q(x) \\approx 0$$\nThis is particularly problematic for activations with long-tail distributions, where small but important values are rounded to zero, effectively eliminating their contribution to subsequent computations.\nOutliers and activation spikes: Neural network weights and activations often contain outliers—rare but large magnitude values. These outliers are especially problematic during inference quantization:\nIf the quantization range is set to accommodate outliers, the majority of values are quantized with poor resolution If the range is set to capture the typical distribution, outliers saturate, potentially causing severe accuracy degradation A single outlier channel in a transformer attention layer can dominate the quantization range, forcing all other channels to use a coarser quantization grid. Research has shown that just 0.1% of outlier features can account for significant quantization error in large language models, particularly in specific attention heads and feed-forward layers.\nReduced precision: rounding errors and numerical instability # Even when values fall within the representable range, limited mantissa bits cause precision loss, where distinct values in $S$ map to the same value in $T$: $$Q(x_1) = Q(x_2) = x_q \\text{ for } x_1 \\neq x_2$$\nThis quantization-induced loss of information manifests in several ways during inference:\nAccumulation of rounding errors: Each quantized operation introduces rounding error, and these errors accumulate as activations flow through the network. In transformer models with 32, 48, or more layers, even small per-layer errors can compound into significant output degradation.\nNumerical instability in critical operations: Operations like normalization (LayerNorm, RMSNorm) and attention softmax are particularly sensitive to precision loss: $$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\nWhen computed in low precision, the exponential function can amplify small quantization errors, and the normalization denominator may lose precision, leading to NaN or inf values. The attention mechanism\u0026rsquo;s dot-product can also overflow when sequence lengths are large, as it computes $QK^T$ where values can grow with $\\sqrt{d_k}$.\nLoss of discriminative ability: In classification or next-token prediction tasks, the model\u0026rsquo;s logits may lose the precision needed to distinguish between similar options. If the quantization step size is larger than the difference between competing logits, the model may produce incorrect predictions. This is especially problematic for language models where probability distributions over large vocabularies require fine-grained discrimination.\nImpact on inference quality # The cumulative effect of quantization error propagates through the network during inference, with deeper layers experiencing compounded errors. For large language models, this can manifest as:\nPerplexity degradation: Increased perplexity on language modeling tasks, indicating worse prediction quality and less confident probability distributions Task accuracy drop: Reduced performance on downstream tasks like question answering, summarization, classification, or reasoning Generation quality issues: Incoherent text, repetition, grammatical errors, factual inconsistencies, or hallucinations during text generation Latency unpredictability: Numerical instabilities may cause divergent behavior, leading to variable or increased generation times Complete failure modes: In extreme cases, the quantized model may produce garbage outputs, get stuck in loops, or encounter numerical errors (NaN/inf) The severity of these issues depends on the target bit-width, the quantization scheme, and the model architecture.\nImportance of controlling quantization error # Effective quantization requires carefully managing the trade-off between compression and accuracy. Key strategies for inference quantization include:\nMixed-precision quantization: Using higher precision for sensitive layers (e.g., first/last layers, attention, layer norms) and lower precision for less sensitive components Outlier management: Handling outliers through techniques like per-channel quantization, group quantization, or explicit outlier extraction and separate handling Granularity selection: Choosing the appropriate level of quantization (per-tensor, per-channel, per-group) to balance accuracy and efficiency The goal is not to eliminate quantization error—which is impossible—but to ensure it remains small enough that model behavior is preserved within acceptable tolerances. For LLM deployment, this typically means maintaining perplexity within 1-2% of the full-precision baseline while achieving 2-4× memory compression and computational speedup.\nIn the following sections, we will explore specific quantization schemes and techniques that address these challenges, enabling practical deployment of large language models.\nTechniques to reduce quantization error # Reducing model quantization errors to enable low precision deployment of LLMs is a very active research area.\nScaling for dynamic range adjustment Due to the smaller dynamic range of low precision data types the parameters are often scaled to fit in a particular range and then quantized afterwards. The granularity of this scaling is an important variable in determining the quantization error. Previous approaches have used tensor-wise, channel-wise scalings while more recently the granularity is further increased by decreasing the block size down to 16 or 32.\nTensor-wise and channel-wise scaling In the tensor-wise approach whole of a tensor is scaled to within the target range, whereas, for the channel-wise scaling the scaling would be along the output channel dimension. The less granular approaches are easier to implement and results in less memory overhead due to the low amount of scales. However, the outliers would have a larger influence on the model; especially outlier within a group may result in underflow of small values in that particular block.\nMicro-scaling formats Micro-scaling formats (MX) are proposed for a better control of outliers. These formats extend the concept of group-wise quantization by applying it at a very fine granularity with a standardized block structure. MX formats use individual scale factors for small blocks of elements (typically 16-32 elements):\n$$x_q^{(i)} = Q\\left(\\frac{x^{(i)}}{s_{\\lfloor i/B \\rfloor}}\\right)$$\nwhere $B$ is the block size and $s_j$ is the shared scale factor for block $j$. The MX formats are supported in latest accelerators such as Blackwell GPUs and AWS Trainium 3.\nTensor-wise vs MX format scaling factors (source docs.nvidia.com). Random Hadamard transformation is a preprocessing technique that redistributes the dynamic range of tensors, making them more uniform and easier to quantize. The Hadamard matrix $H_n$ is an orthogonal matrix with entries $\\pm 1$:\n$$H_n H_n^T = n \\cdot I_n$$\nFor a weight matrix $W$, the transformation applies:\n$$W\u0026rsquo; = D_1 H_d W H_d D_2$$\nwhere $H_d$ is a Hadamard matrix and $D_1, D_2$ are random diagonal scaling matrices with entries $\\pm 1$. Hadamard transforms are orthogonal and spread information uniformly. A single outlier value gets distributed across multiple positions, reducing peak magnitudes. The transformation makes the distribution of values more uniform, better utilizing the quantization codebook. Can be efficiently computed in using the Fast Walsh-Hadamard Transform. By applying the same rotation to inputs and outputs of adjacent layers, the rotations can be absorbed into the weight matrices without changing the model\u0026rsquo;s mathematical function.\nHow to quantize a model? # There are several ways of converting a high precision model to lower precisions. These approaches differ in when quantization is applied, whether calibration data is required, and what computational resources are needed.\nPost-training quantization is the most common approach for inference optimization, where a pre-trained full-precision model is quantized without any additional training or fine-tuning. PTQ is attractive because it requires no access to the original training data or training pipeline and iss computationally inexpensive. The PTQ process typically involves a pass through a small calibration dataset to fine-tune the model for quantization. PTQ may include advanced methods like SpinQuant, Quip, GPTQ that optimize quantization parameters or apply layer-wise fine-tuning to minimize error. The PTQ methods often employ advanced techniques such as RHT to control the quantization error.\nOne-shot quantization can be seen as a subset of PTQ that quantizes the model without requiring any calibration data. It relies solely on the weight statistics themselves to determine quantization parameters. This approach enables immediate quantization of any model without data collection, is useful when calibration data is unavailable or privacy-sensitive, may perform poorly for aggressive quantization (4-bit or lower) or activation quantization. One-shot methods are commonly used for quick deployment scenarios where a small accuracy drop is acceptable in exchange for maximum simplicity. They serve as a baseline for more sophisticated PTQ approaches.\nQuantization-aware training simulates quantization during the training or fine-tuning process, allowing the model to adapt to quantization constraints. During forward passes, weights and activations are quantized, but gradients are computed using straight-through estimators (STE) or other function approximations to enable backpropagation. QAT typically achieves the best accuracy, especially for aggressive quantization (4-bit or lower), but has significant drawbacks for LLM deployment:\nComputational cost: Requires full model training or fine-tuning, which can take days or weeks on large clusters Data requirements: Needs access to large-scale training data, which may be unavailable or proprietary Flexibility: Less flexible for exploring different quantization configurations post-deployment For these reasons, QAT is less commonly used in LLM inference scenarios, where PTQ methods have proven surprisingly effective. However, QAT remains valuable when maximum accuracy at very low bit-widths is critical, or when quantization is planned from the start of model development.\nNote: While this tutorial focuses on inference, understanding QAT is valuable as some publicly released models may have been prepared using QAT techniques.\nPerformance benefits (storage and speedup) # Quantization provides two primary performance benefits for LLM deployment: reduced memory footprint and increased inference throughput. Reduced memory footprint is straight-forward due to the lower bit representations used. Increased inference throughput depends on hardware support.\nPerformance vs. ease of use of different data types. Weight-only quantization: Only model weights are quantized, while activations remain in higher precision (FP16/BF16). This reduces memory footprint and bandwidth but provides limited computational speedup. Weight-activation quantization: Both weights and activations are quantized, enabling greater speedup, but requires more careful calibration to maintain accuracy. Matrix multiply (Matmul): backbone of LLM computation # The speed of inference is often determined by the speed of matrix multiply operations. Recent hardware support matrix multiplications of as low as 4 bit floating point representations. Low precision matrix-multiply accumulate (MMA) operations on (supported) hardware are generally faster compared to high precision ones due to occupying less space on the hardware (i.e. given a fixed area of the chip one can fit more low-precision matmul circuitry). For instance, it is estimated that FP4 matmul is twice faster than FP8 matmul which is twice faster than BF16 matmul on the supported Nvidia chips (e.g. Blackwell).\nDense matmul visiualization (Mishra et al.). Different hardware providers use different approaches for MMA. In particular, Nvidia uses flexible tensor cores, whereas, Google TPUs and AWS Trainium use less flexible but high utility systolic arrays.\nNot all matmuls are equal # Moden LLM architectures are based on transformers that include many different components. Some of these components are more sensitive to low precision operations, these include matmuls in core attention (QK, PV), softmax, input-output embedding. In contrast, MLP layers seem to be relatively robust to quantization while also dominating the FLOP for common sequence length selections.\nFlop distribution across different operations (source: adamcasson.com). However, during inference the sequence length may become so large that FLOP can start to be dominated by the core attention matmuls.\nFlop distribution with increased sequence length (source: adamcasson.com). Recently, quantizing the attention gained more attention, resulting in works that try to use 4/8-bit precision for core attention. This works include FlashAttention3, SageAttention3 and so on. This is an important future direction for model quantization research.\nReferences # Mishra, Asit, et al. \u0026ldquo;Accelerating sparse deep neural networks.\u0026rdquo; arXiv preprint arXiv:2104.08378 (2021). Zhang, Jintao, et al. \u0026ldquo;Sageattention3: Microscaling fp4 attention for inference and an exploration of 8-bit training.\u0026rdquo; arXiv preprint arXiv:2505.11594 (2025). Shah, Jay, et al. \u0026ldquo;Flashattention-3: Fast and accurate attention with asynchrony and low-precision.\u0026rdquo; Advances in Neural Information Processing Systems 37 (2024): 68658-68685. Casson, Adam. \u0026ldquo;Transformer FLOPs.\u0026rdquo; https://www.adamcasson.com/posts/transformer-flops "},{"id":9,"href":"/docs/section-3/subsection-3/index.html","title":"3.3 Chunked Prefill","section":"Systems-Level Inference Optimization","content":" 3.3 Chunked Prefill # "},{"id":10,"href":"/docs/section-1/subsection-4/index.html","title":"1.4 Roofline Analysis","section":" Foundations of Generative Inference","content":" Roofline Analysis # When an algorithm is run on hardware, there are three dominant constraints:\nHow many floating-point operations the chip can perform per second (\u0026quot;FLOPS/s\u0026quot;) How many bytes per second the chip (or system) can move in memory or across an interconnect (\u0026quot;bandwidth\u0026quot;) How much memory you have in total (on-chip, off-chip, network) to hold data. A roofline model blends all of these to give upper and lower bounds on runtime: the time an operation takes cannot be less than the you\u0026rsquo;d get if you were just limited by compute. More precisely,\n$$T_{\\text{math}} = \\frac{\\text{Computation FLOPs}}{\\text{Accelerator FLOPs/s}}, \\hspace{3ex} T_{\\text{comm}} = \\frac{\\text{Communication bytes}}{\\text{Memory Bandwidth bytes/s}}$$\nClearly, an upper bound on the latency of any operation is given by $T_{\\text{math}} + T_{\\text{comm}}$ Moreover, usually, computation can be overlapped with communication. With a perfect overlap, we get a lower bound on the latency of any operation/algorithm to be $\\text{max}\\left(T_{\\text{math}}, T_{\\text{comm}}\\right)$.\nIf communication and computation overlap perfectly, then whenever $T_{\\text{math}} \u0026gt; T_{\\text{comm}}$, the hardware stays fully utilized \u0026ndash; a compute-bound regime. But if $T_{\\text{math}} \u0026lt; T_{\\text{comm}}$, we enter a communication-bound regime, meaning some compute capacity goes unused while the system waits on data transfers.\nThis is often formalized in terms of Accelerator intensity, which is a characteristic of the accelerator, and Computation intensity, which is a function of an operation/algorithm. These are defined as:\n$$\\text{Accelerator intensity} = \\frac{\\text{Accelerator FLOPs/s}}{\\text{Bandwidth bytes/s}}, \\hspace{3ex} \\text{Computation intensity} = \\frac{\\text{Computation FLOPs}}{\\text{Communication bytes}}.$$\nIf an operations has high computation intensity, it will tend to be compute-bound (i.e., you\u0026rsquo;re spending lots of FLOPs per byte of data moved). If it has low intensity, it will tend to be communication or memory bound (you\u0026rsquo;re moving a lot of data for each FLOP).\nVisualization of roofline analysis (credits: How To Scale Your Model) The plot above visualizes a roofline analysis. The X-axis corresponds to the computation intensity of any operation/algorithm, whereas the Y-axis is the achieved FLOPs/s of the operation on the hardware. The roofline itself is defined by two main limiting lines:\nA memory/communication-bandwidth bound line: For low computation intensity, performance is limited by how fast data can be moved (bytes/second) rather than how fast floating-point operations can be done. A compute-bound line: At high computation intensity, performance is limited by the peak FLOPs/sec of the hardware. The intersection of these two types of limits defines a knee or critical intensity. In the figure above, two algorithms: Algo 1 and Algo 2, are plotted against two bandwidths (BW1 and BW2). The regions are color-coder (red = Bandwidth bound at both bandwidths, yellow = bandwidth bound only at the lower bandwidth, green = compute bound at both bandwidths).\nHere\u0026rsquo;s how we can interpret the plot:\nIf our algorithm lies to the left of the knee (i.e., low intensity), we are in the bandwidth/communication-bound regime. In this region, reducing bytes moved, increasing data reuse, improving memory locality, or boosting bandwidth, will improve performance more that increasing raw compute. The FLOPs/s achieved is significantly below the hardware peak. If our point lies to the right of the knee (i.e., high intensity), we are in the compute-bound regime. Here, we are hitting the hardware\u0026rsquo;s peak FLOPs/s (or close to it). Further increasing arithmetic intensity or bandwidth helps only marginally, as we are essentially limited by compute. further gains require more compute capacity (or new precision formats, etc.) The slope of the left part of the plot (bandwidth-bound region) is linear in log-log space, i.e., performance icnreases with computation intensity (more FLOPs per byte = better). Once we hit the knee, the plot flattens because we cannot exceed peak compute. Such a visualization gives us a quick, intuitive way to answer: Will our algorithm be limited by data movement or compute? and What do we need to optimize to get more performance?. Note: In distributed setups, the communicaiton part of the roofline includes the inter-chop links, and thus the knee might shift (because inter-chip bandwidth is often lower than on-chip memory bandwidth). The roofline visualization extends to those cases too.\nReferences # All About Rooflines, How To Scale Your Model. https://jax-ml.github.io/scaling-book/roofline/ "},{"id":11,"href":"/docs/section-2/subsection-4/index.html","title":"2.4 Key-Value Caching","section":"Algorithmic and Modeling-level Inference Optimization","content":" 2.4 Key-Value (KV) Caching # A key optimization for accelerating autoregressive inference in large language models is Key–Value (KV) caching. Transformer-based LLM inference happens in two stages: prefill phase, when the prompt is processed, and decoding phase, when tokens are generated one-by-one. During prefill, each transformer layer computes attention keys K and values V for every input token. Without caching, the model would need to recompute the entire attention stack from scratch at every decoding step—an operation whose cost scales quadratically with the sequence length and quickly prohibits real-time inference. KV caching avoids this waste by storing the computed K and V tensors in memory, so that during decoding, the model only needs to compute attention for new tokens and can reuse the cached representations for all past tokens, avoiding redundant computations. The result is a drastic improvement in per-token latency and a shift in the performance bottleneck from compute to memory bandwidth. More specifically, the quadratic scaling of the attention layer is transformed into a linear scaling at the cost of increased memory utilization.\nCausal attention enables computation reuse # Consider a transformer decoder-only model with $L$ layers, $H$ attention heads, head dimension $d$, and hidden dimension $D = H \\cdot d$. For a token sequence $(x_1, \\ldots, x_T)$, denote the hidden representation entering layer $\\ell$ at index $t$ by $h_t^{(\\ell)} \\in \\mathbb{R}^D$. Each layer $\\ell$ computes: $$Q^\\ell_t = W^\\ell_Q h^\\ell_t, \\qquad K^\\ell_t = W^\\ell_K h^\\ell_t, \\qquad V^\\ell_t = W^\\ell_V h^\\ell_t,$$ where $W^\\ell_Q, W^\\ell_K, W^\\ell_V \\in \\mathbb{R}^{D \\times D}$. During prefill, this computation is done for all $t \\in {1, \\ldots, T}$.\nFor every layer $\\ell$, the cache stores all the past keys and values as:\n$$K^\\ell_{\\le t} = (K^\\ell_1, \\dots, K^\\ell_t) \\in \\mathbb{R}^{t \\times D}$$\n$$V^\\ell_{\\le t} = (V^\\ell_1, \\dots, V^\\ell_t) \\in \\mathbb{R}^{t \\times D}.$$\nThese are appended once during prefill, then incrementally during decoding. This KV cache built during prefill, is re-used repeatedly during decoding as follows. Because of the causal mask, each token\u0026rsquo;s output depends only on representation from previous tokens. Since those earlier representations do not change across decoding steps, the output for a previously generated token remains the same at every iteration, making the repeated computation redundant.\nAs a concrete example, consider the sentence, The quick brown fox as the input sequence. For simplicity, assume each of these words correspond to a single token. Then the representations for the tokens, The, quick, brown, fox are computed during prefill, populating the KV cache. Suppose we generate the next token to be jumps as a continuation of the prompt The quick brown fox. Output representation of the last token, fox, depends only on the tokens up until that point, i.e., The quick brown fox, so its output representation would not change when the new token jumps is taken into consideration \u0026ndash; enabling reuse of the past representations.\nDuring the decoding phase, for a new decoding step at token index $t+1$, we compute $$Q^\\ell_{t+1} = W^\\ell_Q h^\\ell_{t+1}, \\qquad K^\\ell_{t+1} = W^\\ell_K h^\\ell_{t+1}, \\qquad V^\\ell_{t+1} = W^\\ell_V h^\\ell_{t+1},$$ The attention scores for $h^\\ell_{t+1}$ with respect to any previous token $i$, denoted as $\\alpha_{t+1 ,i}$, is computed as follows: $$\\alpha^\\ell_{t+1, i} = \\frac{\\text{exp}\\left(\\frac{(Q^\\ell_{t+1})^\\top K^\\ell_i}{\\sqrt{d}}\\right)}{\\sum_{j=1}^{t+1}\\text{exp}\\left(\\frac{(Q^\\ell_{t+1})^\\top K^\\ell_j}{\\sqrt{d}}\\right)}$$ Note that the above computation reuses the cached $K^\\ell_{\\le t}$ for $i \\leq k$. The contextualized output is computed by reusing $V^\\ell_{\\le t}$ as: $$z_{t+1}^\\ell = \\sum_{i=1}^{t+1}\\alpha_{t+1,i}^\\ell V_i^\\ell$$ Crucially, the model does not recompute $K_i$ or $V_i$ for $i \\leq t$; it loads them directly from the KV cache.\nThis (visualized below) is repeated for every subsequent auto-regressively decoded token, and the representations are appended to the KV cache (which keeps growing with the sequence length).\nEvery decoding step caches and reuses previously computed keys and values (red). Only blue blocks are new (credits: Pierre Lienhart) Computation complexity with and without KV cache # In the absence of KV cache, at each decoding step, attention is recomputed over all previous tokens. Hence, for a sequence length of $t$, we have, $$\\text{Cost (without KV cache)} = \\sum_{u=1}^{t+1}\\text{O}(uD) = \\text{O}(t^2D)$$\nHowever, with KV cache, at time $t+1$, for each layer $\\ell$, we need to query $Q_{t+1}^\\ell$ , key $K_{t+1}^\\ell$, and value $V_{t+1}^\\ell$ only for the last token $t+1$, i.e., a compute cost of $\\text{O}(D^2)$. Additionally, we need to do another single attention matvec with $K_{\\le t}^\\ell$, which incurs a cost of $\\text{O}(tD)$. Therefore, $$\\text{Cost (with KV cache)} = \\text{O}(tD + D^2).$$ This reduces the quadratic growth with repect to $t$ to linear at the price of additional memory.\nMemory footprint of KV cache # For an LLM with $L$ layers, for each layer $\\ell$, and token position $i \\le T$, the cache stores $K_i^\\ell \\in \\mathbb{R}^{D}$ and $V_i^\\ell \\in \\mathbb{R}^{D}$, each stored in some preciison $b$ bytes (e.g., FP16\u0026ndash;\u0026gt; 2 Bytes, FP8 \u0026ndash;\u0026gt; 1 Byte). Therefore, the total KV cache size is $$\\text{KV Cache memory} = 2b\\cdot L \\cdot T \\cdot D$$ This linear dependence on sequence length $T$ is the primary reason why very long-context models need aggressive KV compression.\nIncremental update of KV cache: At each decoding step, the KV cache is updated as follows: $$K^\\ell_{\\le t+1} = \\left[K^\\ell_{\\le t} \\hspace{2px}\\vert\\hspace{2px} K^\\ell_{t+1}\\right] \\quad \\text{and} \\quad V^\\ell_{\\le t+1} = \\left[V^\\ell_{\\le t} \\hspace{2px}\\vert\\hspace{2px} V^\\ell_{t+1}\\right].$$ This append operation is $\\text{O}(D)$ per layer, trivial compared to the attention read cost.\nRuntimes like vLLM optimize this step with Paged KV layouts so these appends do not cause memory fragmentation or reallocation.\nPrefill vs. Decoding # During prefill, full self-attention is computed over $T_{\\text{input}}$ (input sequence length) tokens which involves multiplying query and key matrices of shape $T_{\\text{input}} \\times D$ with $D$ as the partition dimension. Moreover, computing the values involved multiplying an attention score matrix of shape $T_{\\text{input}} \\times T_{\\text{input}}$ with the value matrix of shape $T_{\\text{input}} \\times D$. So, the cost is given by $$\\text{Cost (prefill)} = \\text{O}(LDT_{\\text{input}}^2),$$ which is dominated by large GEMM operations. Succinctly stated, prefill is compute-bound and this affect the time-to-first-token (TTFT).\nFor decoding each new token $t+1$, each layer $\\ell$ needs to read $K^\\ell_{\\le t} \\in \\mathbb{R}^{t \\times D}$ and $V^\\ell_{\\le t} \\in \\mathbb{R}^{t \\times D}$. This adds up to $2bLtD$ bytes across all layers. This is why autoregressive decoding in memory-bandwidth bound as attention repeatedly streams through the entire cached context. In this decoding regime, runtime is not limited by FLOPs but by the memory-bandwidth, which is proportional to $T$. This affects the decoding throughput, measured as output-tokens-per-second (OTPS).\nAs a consequence, often prefill and decoding regimes require separately optimized kernels to optimize for overall latency.\nConnections with efficient attention: MHA vs. GQA # During autoregressive decoding, generating tokens sequentially isn’t constrained by compute capacity; the real bottleneck is the memory bandwidth required to stream the KV cache from HBM. In Multi-Head Attention (MHA), each query head has its own key and value heads (e.g., 64 Q, 64 K, 64 V), whereas, in Grouped Query Attention (GQA), multiple query heads share a smaller number of K/V heads (e.g., 64 Q but only 8 K and 8 V). This design make s abig difference. By drastically reducing the number of K/V heads, GQA shrinks the KV cache by approximately 4-8x in many models. A smaller cache means dramatically less data to stream from HBM at each decoding steps. As a consequence, the memory bandwidth bottleneck is somewhat alleviated. This is a central reason why models like Llama-2/3, Mistral achieved faster decoding over their predecessors. Note, however, GQA is a training-time optimization, i.e., the model architecture is defined beforehand, and models with GQA need to be trained from scratch.\nReferences # LLM Inference Series: 3. KV caching explained, Pierre Lienhart, 2023 https://medium.com/@plienhar/llm-inference-series-3-kv-caching-unveiled-048152e461c8 LLM Inference Series: 4. KV caching, a deeper look, Pierre Lienhart, 2024 https://medium.com/@plienhar/llm-inference-series-4-kv-caching-a-deeper-look-4ba9a77746c8 "},{"id":12,"href":"/docs/section-3/subsection-4/index.html","title":"3.4 Disaggregated Inference","section":"Systems-Level Inference Optimization","content":" 3.4 Disaggregated Inference # "},{"id":13,"href":"/docs/section-1/subsection-5/index.html","title":"1.5 Measuring Inference Efficiency","section":" Foundations of Generative Inference","content":" Measuring Inference Efficiency # Evaluating the performance of an LLM during inference requires a clear understanding of both speed and cost efficiency. While model quality and reasoning capability are crucial (and invariably non-negotiable), the practical utility of an LLM, especially in production systems, often hinges on how quickly and economically it can generate responses. To quantify this, several key metrics are used to capture different aspects of inference behavior. These are visualized and described in details below.\nVisualization of performance metrics TTFT and ITL Latency (Time-to-First-Token) # Time-to-first-token (TTFT) measures the elapsed tie between submitting a prompt to an LLM and receiving the first generated token in response. It captures the model’s initial latency, encompassing tokenization, prompt encoding, etc., and the first forward pass through the network. TTFT is especially important for interactive or streaming applications such as chatbots or autocomplete systems, where perceived responsiveness directly affects user experience.\nFor example, consider the prompt: \u0026ldquo;The quick brown fox jumps over the\u0026rdquo;. In a next-word-prediction task, the model might output the next token \u0026ldquo;lazy\u0026rdquo;. If it takes 480 ms from sending the prompt to receiving \u0026ldquo;lazy\u0026rdquo;, then the TTFT is 480 ms, regardless of how quickly subsequent tokens (e.g., \u0026ldquo;dog\u0026rdquo;, \u0026ldquo;.\u0026rdquo;) are generated afterward. TTFT is a crucial indicator of responsiveness, especially for interactive or low-latency applications, where users perceive delay before the model begins to speak.\nBecause the prefill stage processes the entire input sequence in a single pass before any decoding begins, TTFT is generally compute-bound, i.e., its latency is dominated by dense matrix multiplications and attention operations over long input sequences.\nOutput Speed (Tokens per Second) # Output tokens per second (OTPS) quantifies how quickly an LLM generates tokens after the first token has appeared. It is typically measured as the average number of tokens produced per second (tokens/sec) during the streaming phase of inference. High output speed is crucial for long-form generation and large-scale deployments, where total inference time and serving cost scale with the number of generated tokens. OTPS can also be measured by its inverse, inter-token latency (ITL).\nFor example, continuing from the previous prompt, \u0026ldquo;The quick brown fox jumps over the\u0026rdquo;. If the model generates the next 6 tokens (\u0026quot;lazy dog and runs away.\u0026quot;) in 0.3 seconds, the output speed is 20 tokens/sec. This indicates that, after the initial TTFT, the model sustains a generation rate of 20 tokens per second until completion.\nUnlike prefill, decoding is incremental, i.e., each step depends on the previously generated token, and is therefore often bandwidth-bound rather than compute-bound. The need to repeatedly access and update large key–value (KV) caches for attention makes memory throughput and cache locality key determinants of OTPS. High OTPS is especially important for long-form generation and large-scale serving workloads, where sustained decoding performance dictates overall system efficiency.\nThroughput # While Time-to-First-Token (TTFT) and Output Tokens per Second (OTPS) describe the latency and generation speed of a single inference request, real-world LLM deployments rarely serve one user at a time. Production systems such as chat services, retrieval-augmented APIs, or model endpoints handle many concurrent prompts arriving continuously, each with varying sequence lengths and response demands. In such scenarios, performance depends not only on how fast one request completes, but on how efficiently the entire system processes multiple requests simultaneously while utilizing available compute resources.\nThroughput captures this aggregate behavior by measuring the total number of tokens generated per second across all concurrent inferences. It reflects how effectively the model server converts hardware capacity into useful output, and is often reported as tokens per second per device or tokens per second per deployment. High throughput indicates strong hardware utilization and scheduling efficiency, typically achieved through techniques such as dynamic batching, asynchronous queuing, and pipeline or tensor parallelism.\nIn deployment benchmarks (e.g., vLLM, TensorRT-LLM), throughput is often measured over the total wall-clock time, which does include TTFT periods across concurrent requests. It is given by\n$$\\text{Throughput}_{\\text{system}} = \\frac{\\text{Total tokens generated across all requests}}{\\text{Total wall-clock duration}}$$\nThroughput differs from TTFT and OTPS in scope and interpretation. TTFT measures responsiveness—how quickly the first token is produced—while OTPS measures streaming speed once generation begins. Throughput, in contrast, represents system-level efficiency under concurrency. Although TTFT is not directly included in throughput calculations, long TTFTs can still reduce throughput by delaying when generation can begin. Systems that overlap the prefill and decoding phases, such as those employing continuous batching (e.g., vLLM) maintain high throughput even when TTFT varies across requests.\nFor example, suppose a model server processes 16 simultaneous prompts, each generating 100 tokens over 4 seconds. The system outputs a total of 1,600 tokens, giving a throughput of 400 tokens/sec. Even if each request has a TTFT of 600 ms, the aggregate throughput remains high because token generation for later requests overlaps with the TTFT of earlier ones. This illustrates how TTFT governs perceived user latency, while throughput governs aggregate system efficiency and cost in large-scale LLM serving.\nReferences # Artificial Analysis: Understand the AI landscape to choose the best model and provider for your use case https://artificialanalysis.ai/ "},{"id":14,"href":"/docs/section-2/subsection-5/index.html","title":"2.5 Speculative Decoding","section":"Algorithmic and Modeling-level Inference Optimization","content":" 2.5 Speculative Decoding # "},{"id":15,"href":"/docs/section-3/subsection-5/index.html","title":"3.5 Multi-LoRA serving","section":"Systems-Level Inference Optimization","content":" 3.5 Multi-LoRA serving # Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) technique used to adapt large language models (LLMs) for specific tasks at a low cost. Compared to full parameter fine-tuning, which retrains all of the LLM parameters, LoRA freezes the original model weights and injects a small number of new, trainable parameters (adapters) into the model\u0026rsquo;s layers. These adapters consist of two small matrices that are updated instead of the full weight matrix.\nThis method provides several key advantages, including computational and memory efficiency, as the size of LoRA adapters is typically less than 1% of the base model size.\n"},{"id":16,"href":"/docs/section-2/subsection-6/index.html","title":"2.6 Knowledge Distillation","section":"Algorithmic and Modeling-level Inference Optimization","content":" 2.6 Knowledge Distillation # Knowledge distillation (KD) refers to a principled way to transfer the capabilities of a large teacher model into a smaller, faster student model \u0026ndash; often achieving substantial reductions in latency, memory footprint, and cost. In inference optimization pipelines, KD can often be used in conjunction with other model compression techniques like quantization. The following schematic captures the general idea of KD:\nKnowledge Distillation In the above figure, for each example $x$ in the dataset, the teacher generates a predictive distribution $p_t(y \\vert x)$ over outputs $y$, and the student generates its own distribution $p_s(y \\vert x)$. The distillation loss $\\ell(p_t, p_s)$ trains the student to mimic the teacher.\nA smaller model distilled from a large teacher consistently outperforms the same model trained from scratch with the same data budget. This happens for several fundamental reasons \u0026ndash; rooted in optimization, representational capacity, data distributions, and the nature of supervision signals, which are intuitively described below:\nThe teacher provides a richer learning signal than one-hot labels. Since the soft distribution over labels is a much smoother optimization landscape, it allows the small student model to learn subtle patterns it could never discover from one-hot labels, when trained from scratch. Consequently, the effective sample complexity is reduced as the teacher provides supervisory information for every token, not just the correct one. Big models can learn higher-quality representations that small models can imitate but cannot discover. Rather than needing to infer long-range semantics or complex reasoning from raw text, the student only needs to approximate the teacher’s function, which is a simpler mapping than learning the function from scratch. It is often useful to consider the following analogy \u0026ndash; a college student can learn calculus from a professor in months, but would not reinvent calculus on their own even in years. Types of knowledge distillation # While KD can encompasses diverse strategies, there a three major categories used for LLMs, which are described below:\nLogit/Soft-label distillation # This is the classic form of KD where the student learns directly from the teacher\u0026rsquo;s probability distribution over tokens (aka logits). Given an input $x$, let $z_t$ and $z_s$ be the logits of the teacher and student, respectively. Logits here refer to the raw, unnormalized scores before softmax. Then, $p_t = \\sigma(z_t/T)$ and $p_s = \\sigma(z_s/T)$ are the corresponding probability distributions over the the token, where $\\sigma$ is the Softmax function, and $T$ is the temperature for sampling. For some $0 \u0026lt; \\alpha \\le 1$, the distillation loss used by the student is: $$L_{\\rm KD} = (1 - \\alpha)L_{\\rm CE}(z_s, y) + \\alpha T^2L_{\\rm KL}(p_t \\parallel p_s).$$ Here, $L_{\\rm CE}$ is the cross-entropy loss between the student\u0026rsquo;s logits and the ground truth labels $y$, and $L_{\\rm KL}$ is the KL divergence between the teacher\u0026rsquo;s and the student\u0026rsquo;s logits. $\\alpha$ blends the two types of loss. Note: $\\alpha = 1$ refers to pure logit-level distillation. This type of KD ignores the teacher\u0026rsquo;s intermediate computation, and only the logits from the teacher (which may be available through an API) suffice. This is the most common type of KD, and is primarily what Minitron uses.\nSequence-level distillation # In sequence-level KD, instead of (or in addition to) matching logits, the student is trained on text sequences generated by the teacher using a standard cross-entropy training loss. This is sometimes also referred to as supervised-fine-tuning with synthetic data. Given an input $x$, the teacher generates a compute output sequence: $$y_t = (y_{t,1}, y_{t,2}, \\ldots, y_{t,L})$$ of length $L$. the student is trained to predict this tacher generated sequence as if it were the ground-truth using the training objective: $$L_{\\rm seq} = -\\sum_{i=1}^{L}\\log p_s(y_{t,i} \\vert x, y_{t, \u0026lt;i})$$ Note that this uses teacher-generate labels instead of human-provided labels.\nSequence-level KD is effective because human datasets, in general, often contain noise and stylistic variance. On the other hand, teacher outputs from a single model produces a coherent, consistent behavior pattern, giving the student clean suppervision. Moreover, teacher-generated outputs can be produced for large datasets cheaply (without human labeling), providing abundant training data.\nWhile logit-level KD transfers token-level distributions from the teacher to student, sequence-level KD transfers semantic structures such as reasoning traces and formatting preferences. This enables compressing models beyond logit fidelity since even though a small student cannot match the teacher\u0026rsquo;s softmax landscapes for reasoning tasks, it can still imitate the teacher\u0026rsquo;s reasoning patterns via generates sequences.\nQuite importantly, sequence-level KD is especially beneficial for closed models (e.g., GPT-5, Claude, Gemini, etc.) because unlike open models, they often do not allow access to logits, hidden states, or internal probabilities.\nFeature-level/Intermediate representation distillation # In feature-level KD, the student is trained to match the teacher\u0026rsquo;s hidden states, attention maps, or intermediate embeddings. This can lead to dramatically stronger students, but at a significantly higher computational cost. For example, to match the attention-map, a Frobenius norm error loss like $$L_{\\rm attn}= \\sum_{(l,m)} \\lVert A_s^{(m)} - A_t^{(l)} \\rVert_F^2$$ may be used. Here, the attention scores of the $m^{\\rm th}$ layer of the student is mapped to the $l^{\\rm th}$ layer of the teacher.\nThe intuition behind why feature-level KD is effective lies in the fact that LLMs build hierarchical neural representations. For instance, lower-layers learn lexical and morphological features, mid-layers learn syntax and symantics, wheareas higher-layers learn abstraction and world knowledge. While it is difficult for a small student to learn these tructures from scratch, it can learn to imitate them from the teacher.\nWhile very effective, it should be noted that feature-level KD is rare for distilling knowledge from large-scale LLMs because the computation and memory cost explode. To distill hidden states for a sequence length $S$, hidden dimension $d$, and across $L$ layers, all hidden states must be stoed, which requires $O(LSd)$ memory, which blows up. Hence, logit-level KD and sequence-level KD are more popular for large scale LLMs. Feature-level KD is more common in vision models and smaller text models of relatively small size ($\\leq 3{\\rm B}$ params).\nReferences # Sreenivas et. al., LLM Pruning and Distillation in Practice: The Minitron Approach https://arxiv.org/abs/2408.11796 Hinton, Vinyals, Dean, Distilling the Knowledge in a Neural Network, NeurIPS 2014 https://arxiv.org/abs/1503.02531 Romero et. al., FitNets: Hints for Thin Deep Nets, ICLR 2015 https://arxiv.org/abs/1412.6550 Huang and Wang, Like What You Like: Knowledge Distill via Neuron Selectivity Transfer, 2017 https://arxiv.org/abs/1707.01219 Zagoruyko and Komadakis, Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer, ICLR 2017 https://arxiv.org/abs/1612.03928 "},{"id":17,"href":"/docs/section-3/subsection-6/index.html","title":"3.6 Compute Graph Optimization","section":"Systems-Level Inference Optimization","content":" 3.6 Compute Graph Optimization # Piecewise CUDA Graph # Efficiently handling varying input token counts and context lengths across execution steps presents a significant challenge in LLM inference. To address this, vLLM leverages torch.compile to capture CUDA Graphs across multiple bucket sizes, enabling optimized execution paths for different input configurations.\nThis graph capture mechanism proves highly effective for token-wise operators—operations that process each token independently, such as RoPE and RMSNorm. These operators exhibit predictable computational patterns that can be captured and replayed efficiently through CUDA Graphs.\nHowever, cross-token operators present a different challenge. The attention module, which computes relationships between tokens, must execute in eager mode because it depends on multi-dimensional input shapes that vary dynamically. Exhaustively enumerating all possible shape combinations during compilation would be impractical, making CUDA Graph capture infeasible for these operators.\nThe following diagram illustrates the Piecewise CUDA Graph architecture in vLLM:\nToken-wise operators like QKV projection and MLP layers are captured with torch.compile for CUDA Graph execution (light green). Cross-token operators like the attention module execute in eager mode (light yellow). (credits: 8th vLLM Meetup Slides) By separating token-wise and cross-token operators, the piecewise CUDA Graph approach achieves high-performance execution through CUDA Graphs where possible, while maintaining the flexibility to handle dynamic shapes in attention operations. This hybrid execution model delivers the performance benefits of graph optimization without imposing restrictive shape constraints on the system.\nPersistent Kernel # While CUDA Graphs reduce both CPU overhead and kernel launch latency to some extent, persistent kernels take optimization further by producing megakernels—large, unified kernels that eliminate kernel launch overhead entirely and remove pipeline bubbles between operations.\nThe Mirage Persistent Kernel (MPK) framework implements this approach through a sophisticated two-stage process. First, the MPK compiler analyzes the computation graph and decomposes it into a fine-grained task graph, where each task represents a unit of work with explicit dependencies on other tasks. This task-level representation enables more flexible scheduling and resource utilization than traditional kernel-by-kernel execution.\nSecond, the MPK runtime orchestrates execution through a worker-scheduler architecture. Worker threads actively poll the task queue and execute tasks as they become available. Meanwhile, scheduler threads monitor task dependencies and emit activation events when all prerequisites for a task are satisfied. This event-driven coordination ensures tasks execute as soon as their dependencies are met, maximizing hardware utilization and minimizing idle time.\nThe MPK runtime executes a task graph within a single megakernel, with workers processing tasks and schedulers managing dependencies. (credits: Compiling LLMs into a MegaKernel: A Path to Low-Latency Inference, Zhihao Jia) By consolidating multiple operations into a single persistent megakernel, MPK eliminates the overhead of launching individual kernels and the synchronization costs between them, resulting in substantially reduced inference latency for LLM workloads.\n"},{"id":18,"href":"/docs/section-3/subsection-7/index.html","title":"3.7 Kernel Fusion","section":"Systems-Level Inference Optimization","content":" 3.7 Kernel Fusion # Memory Hierarchy # Modern accelerators (GPUs, TPUs, AWS Trainium/Inferentia) employ a multi-level memory hierarchy, with memories organized from fastest/smallest to slowest/largest:\nRegisters / On-chip SRAM (e.g., GPU shared memory, NeuronCore SBUF): Highest bandwidth (~20x higher than HBM), lowest latency, but limited capacity (tens of MB per compute unit) Device Memory / HBM: Large capacity (tens of GB), high bandwidth compared to CPU memory, but still orders of magnitude slower than on-chip SRAM Host Memory (CPU DRAM): Largest capacity, lowest bandwidth from accelerator\u0026rsquo;s perspective The memory wall refers to the growing gap between compute throughput and memory bandwidth. While compute capabilities have scaled dramatically (e.g., a single AWS Trn2 NeuronCore delivers ~335 TFLOPs BF16), memory bandwidth has not kept pace.\nExample: NeuronCore (AWS Trainium) Programming Model # AWS Trainium\u0026rsquo;s NeuronCore architecture provides a concrete example of software-managed memory hierarchy. Unlike GPUs with hardware-managed caches, NKI (Neuron Kernel Interface) requires programmers to explicitly control data movement between HBM and on-device memory buffers.\nNeuronCore memory hierarchy showing capacity and bandwidth at each level NeuronCore exposes a 4-level memory hierarchy:\nMemory Level Capacity Bandwidth PSUM (on-chip) ~2 MB ~10 TB/sec SBUF (on-chip) ~25 MB ~10 TB/sec HBM (device) ~50 GB ~0.5 TB/sec per NC Host DRAM ~1 TB ~16 GB/sec The key observation: on-chip SBUF provides 20x higher bandwidth than HBM (10 TB/s vs 0.5 TB/s). With no hardware cache to automatically manage data movement, the programmer must explicitly manage data transfers between memory tiers (shown as \u0026ldquo;Refill\u0026rdquo; and \u0026ldquo;Spill\u0026rdquo; in the diagram). This explicit memory model makes kernel fusion essential: fusing multiple operations keeps intermediate data in fast SBUF instead of incurring expensive HBM round-trips between each operation.\nArithmetic Intensity # Due to the memory wall, many deep learning operations are memory-bound rather than compute-bound — the accelerator spends more time waiting for data than performing arithmetic. Arithmetic intensity measures how much computation is performed per byte of memory transferred:\n$$\\text{Arithmetic Intensity} = \\frac{\\text{FLOPs}}{\\text{Bytes Transferred}}$$\nThe units are FLOPs/byte. This metric determines whether an operation is compute-bound or memory-bound via the roofline model. Every accelerator has a ridge point defined by:\n$$\\text{Ridge Point} = \\frac{\\text{Peak Compute (FLOPs/s)}}{\\text{Peak Bandwidth (Bytes/s)}}$$\nFor example, an AWS Trn2 NeuronCore has ~335 TFLOPs (BF16) and ~1.45 TB/s HBM bandwidth, giving a ridge point of ~230 FLOPs/byte. Operations with arithmetic intensity below this threshold are memory-bound; those above are compute-bound. Most LLM inference operations—especially during the decode phase where batch sizes are small—have low arithmetic intensity and are memory-bound. This is why kernel fusion is critical: by keeping intermediate data in fast on-chip memory, we reduce memory traffic and shift operations closer to being compute-bound.\nWhat is Kernel Fusion? # Kernel fusion is a compiler/runtime optimization that combines multiple operations into a single kernel to minimize memory traffic between the accelerator\u0026rsquo;s on-chip memory and device memory (HBM).\nWithout fusion, each operation in a computation graph:\nReads input tensors from HBM to on-chip memory Performs computation Writes output tensors back to HBM With fusion, intermediate results are kept in fast on-chip memory (SRAM/registers), eliminating redundant HBM round-trips. This is particularly impactful because:\nOn-chip SRAM provides ~20x higher bandwidth than HBM Reducing HBM accesses directly improves performance for memory-bound operations Fused kernels can leverage tiling to process large tensors in cache-friendly blocks Standard attention without fusion: each operation (S = Q × Kᵀ, P = softmax(S), O = P × V) requires loading inputs from HBM and storing outputs back to HBM, resulting in multiple expensive memory round-trips. Arithmetic intensity of standard attention. For sequence length $N$, head dimension $d$, and FP16 precision (2 bytes per element), we can compute the memory traffic for each step:\n$S = QK^T$: Read $Q$ ($2Nd$ bytes), read $K$ ($2Nd$ bytes), write $S$ ($2N^2$ bytes) $P = \\mathrm{softmax}(S)$: Read $S$ ($2N^2$ bytes), write $P$ ($2N^2$ bytes) $O = PV$: Read $P$ ($2N^2$ bytes), read $V$ ($2Nd$ bytes), write $O$ ($2Nd$ bytes) Total memory traffic: $8Nd + 8N^2$ bytes. The two matrix multiplications contribute $4N^2d$ FLOPs, while softmax adds $\\sim 5N^2$ FLOPs. This gives:\n$$\\text{Arithmetic Intensity} = \\frac{4N^2d}{8Nd + 8N^2} = \\frac{Nd}{2(d + N)}$$\nFor $N = 4096$ and $d = 128$: arithmetic intensity $\\approx 62$ FLOPs/byte — well below Trn2\u0026rsquo;s ridge point of ~230 FLOPs/byte. Standard attention is memory-bound, making it an ideal candidate for kernel fusion.\nFusion Techniques # Operator Fusion # On NeuronCore, each operation (matmul, activation, normalization) is typically implemented as a separate NKI kernel. Each kernel reads inputs from HBM into SBUF, computes, and writes outputs back to HBM. For a sequence of $k$ operations on a tensor of size $n$ bytes, this means $2kn$ bytes of HBM traffic — each intermediate result is spilled to HBM and then refilled.\nOperator fusion combines multiple operations into a single kernel. The fused kernel loads inputs once, performs all computations while keeping intermediates in SBUF, and writes only the final output. This reduces HBM traffic to $2n$ bytes — a $k\\times$ reduction.\nExample: Consider computing $y = \\mathrm{GELU}(Wx + b)$. Without fusion:\nKernel 1: Refill $W, x$ → compute $z = Wx$ → spill $z$ to HBM Kernel 2: Refill $z, b$ → compute $z\u0026rsquo; = z + b$ → spill $z\u0026rsquo;$ to HBM Kernel 3: Refill $z\u0026rsquo;$ → compute $y = \\mathrm{GELU}(z\u0026rsquo;)$ → spill $y$ to HBM With fusion, a single kernel computes $y = \\mathrm{GELU}(Wx + b)$ directly: refill $W, x, b$ once, compute everything with $z, z\u0026rsquo;$ in SBUF, spill $y$ once. Memory traffic drops from $6n$ to $2n$ bytes.\nThe key constraint: all intermediate data must fit in SBUF. This works well when intermediates are the same size as inputs (element-wise ops, reductions), but fails when operations expand data size.\nTiling # When intermediate data exceeds SBUF capacity, tiling partitions tensors into blocks that fit in SBUF, applying fusion within each tile.\nExample: For $C = g(f(A))$ where $B = f(A)$ is too large for SBUF:\nUnfused: Compute all of $B$, spill to HBM, then compute all of $C$ by refilling $B$ Tiled: For each tile $i$: refill $A_i$ → compute $B_i$ in SBUF → compute $C_i$ → spill $C_i$ The intermediate $B_i$ never touches HBM. This achieves the same $k\\times$ memory traffic reduction as operator fusion, but requires that $C_i$ depends only on $A_i$ (the computation must be tileable).\nFlash Attention and Mathematical Fusion # Why Standard Fusion Fails for Attention # Standard self-attention computes: $$S = QK^T, \\quad P = \\mathrm{softmax}(S), \\quad O = PV$$\nWithout fusion, each step requires HBM round-trips:\nLoad $Q, K$ from HBM → compute $S$ → store $S$ to HBM Load $S$ from HBM → compute $P$ → store $P$ to HBM Load $P, V$ from HBM → compute $O$ → store $O$ to HBM The attention matrix $S$ has size $N \\times N$. For sequence length $N = 8192$, this is 67M elements per head — far exceeding SBUF capacity (~25 MB). Can we tile to avoid materializing the full matrix?\nThe softmax barrier. Consider computing one row of output $o_i = \\sum_j p_{ij} v_j$ where $p_{ij} = [\\mathrm{softmax}(s_i)]_j$. If we tile over $K$ and $V$ (processing columns in blocks), we compute partial scores $s_{ij}$ for $j \\in B_k$ in each block. But softmax couples all columns:\n$$[\\mathrm{softmax}(s_i)]_j = \\frac{e^{s_{ij}}}{\\sum_{k=1}^{N} e^{s_{ik}}}$$\nThe denominator requires all $N$ scores in the row. We cannot compute any $p_{ij}$ until we have seen every $s_{ik}$.\nNumerical stability makes it worse. Naive softmax overflows when scores are large. The numerically stable version subtracts the row maximum:\n$$m_i = \\max_j s_{ij}, \\quad \\ell_i = \\sum_{j=1}^{N} e^{s_{ij} - m_i}, \\quad p_{ij} = \\frac{e^{s_{ij} - m_i}}{\\ell_i}$$\nThis requires two passes: first to find $m_i$, then to compute exponentials and sum. Both $m_i$ and $\\ell_i$ depend on all $N$ elements, so we must materialize the entire row of $S$ before computing any element of $P$.\nThe fusion blocker. This creates a dependency chain that prevents tiling:\nCannot compute $p_{ij}$ without knowing $m_i$ (need full row of $S$) Cannot compute $m_i$ without all $s_{ij}$ (need to finish $S = QK^T$ first) Result: we must write the full $N \\times N$ matrix $S$ to HBM, then read it back for softmax. Standard fusion and tiling cannot break this logical dependency.\nOnline Softmax: The Mathematical Transform # The key innovation is online softmax — a mathematically equivalent reformulation that computes numerically stable softmax incrementally in a single pass, removing the fusion blocker.\nRecall the numerically stable softmax requires two passes: $$m = \\max_j s_j, \\quad \\ell = \\sum_j e^{s_j - m}, \\quad [\\mathrm{softmax}(s)]_j = \\frac{e^{s_j - m}}{\\ell}$$\nOnline softmax processes elements in blocks, maintaining running statistics $(m, \\ell)$ that can be updated incrementally while preserving numerical stability.\nDerivation: Suppose we have processed blocks $1, \\ldots, i$ with running max $m^{(i)}$ and running sum $\\ell^{(i)} = \\sum_{j=1}^{i} \\sum_{k \\in B_j} e^{s_k - m^{(i)}}$. When we encounter block $i+1$ with elements $\\lbrace s_k \\rbrace_{k \\in B_{i+1}}$:\nUpdate max: $$m^{(i+1)} = \\max(m^{(i)}, \\max_{k \\in B_{i+1}} s_k)$$\nUpdate sum: The previous sum $\\ell^{(i)}$ was computed with offset $m^{(i)}$, but we need all terms to use the new offset $m^{(i+1)}$ for numerical stability. Using $e^{s_k - m^{(i)}} = e^{s_k - m^{(i+1)}} \\cdot e^{m^{(i+1)} - m^{(i)}}$, we rescale: $$\\ell^{(i+1)} = e^{m^{(i)} - m^{(i+1)}} \\cdot \\ell^{(i)} + \\sum_{k \\in B_{i+1}} e^{s_k - m^{(i+1)}}$$\nThe rescaling factor $e^{m^{(i)} - m^{(i+1)}} \\leq 1$ (since $m^{(i+1)} \\geq m^{(i)}$) ensures all exponents remain non-positive, maintaining numerical stability throughout.\nCorrectness: After processing all blocks, $\\ell^{(\\mathrm{final})} = \\sum_k e^{s_k - m^{(\\mathrm{final})}}$, which is exactly the denominator needed for numerically stable softmax.\nExtending to attention output: For $o = \\sum_k [\\mathrm{softmax}(s)]_k \\cdot v_k$, we maintain a running (unnormalized) output: $$o^{(i)} = \\sum_{j=1}^{i} \\sum_{k \\in B_j} e^{s_k - m^{(i)}} \\cdot v_k$$\nWhen processing block $i+1$, rescale the previous output to use the new max and add new contributions: $$o^{(i+1)} = e^{m^{(i)} - m^{(i+1)}} \\cdot o^{(i)} + \\sum_{k \\in B_{i+1}} e^{s_k - m^{(i+1)}} \\cdot v_k$$\nAfter all blocks: $o^{(\\mathrm{final})} / \\ell^{(\\mathrm{final})} = \\sum_k [\\mathrm{softmax}(s)]_k \\cdot v_k$ — mathematically identical to standard attention, but computed in a single pass with guaranteed numerical stability.\nFlash Attention: Fused Attention Kernel # Flash Attention combines online softmax with tiling to fuse all attention operations into a single kernel:\nAlgorithm: Flash Attention\nInput: $Q, K, V \\in \\mathbb{R}^{N \\times d}$, block sizes $B_r, B_c$ where $B_r \\cdot d + B_c \\cdot d + B_r \\cdot B_c \\leq M$\nOutput: $O = \\mathrm{softmax}(QK^T)V$\nDivide $Q$ into row blocks $Q_1, \\ldots, Q_{N/B_r}$ of size $B_r \\times d$ Divide $K, V$ into row blocks of size $B_c \\times d$ for $i = 1$ to $N/B_r$ do Initialize: $m_i \\leftarrow -\\infty$, $\\ell_i \\leftarrow 0$, $O_i \\leftarrow 0$ for $j = 1$ to $N/B_c$ do Load $Q_i, K_j, V_j$ from HBM to SBUF Compute $S_{ij} \\leftarrow Q_i K_j^T$ Compute $\\tilde{m}{ij} \\leftarrow \\mathrm{rowmax}(S{ij})$, $m_i^{\\mathrm{new}} \\leftarrow \\max(m_i, \\tilde{m}_{ij})$ Compute $\\tilde{P}{ij} \\leftarrow \\exp(S{ij} - m_i^{\\mathrm{new}})$ Update $\\ell_i \\leftarrow e^{m_i - m_i^{\\mathrm{new}}} \\ell_i + \\mathrm{rowsum}(\\tilde{P}_{ij})$ Update $O_i \\leftarrow e^{m_i - m_i^{\\mathrm{new}}} O_i + \\tilde{P}_{ij} V_j$ Set $m_i \\leftarrow m_i^{\\mathrm{new}}$ end for Write $O_i \\leftarrow O_i / \\ell_i$ to HBM end for return $O$ Flash Attention fuses all operations into a single kernel: Q, K, V are refilled once from HBM, intermediate results S and P stay in fast on-chip SBUF, and only the final output O is spilled back to HBM. Performance Analysis # IO Complexity of Standard Attention: Reading $Q, K, V$ costs $O(Nd)$. Materializing and reading back the $N \\times N$ matrices $S$ and $P$ costs $O(N^2)$. Total: $O(Nd + N^2)$.\nIO Complexity of Flash Attention: We count HBM accesses for each tensor:\n$Q$: Each block $Q_i$ ($B_r \\times d$ elements) is loaded once per outer loop iteration → $(N/B_r) \\cdot B_r d = Nd$ total $K, V$: Each block $K_j, V_j$ ($B_c \\times d$ elements) is loaded once per inner loop, repeated for all outer iterations → $(N/B_r) \\cdot (N/B_c) \\cdot B_c d = N^2 d / B_r$ each $O$: Each block $O_i$ is written once → $Nd$ total Total HBM accesses: $Nd + 2 \\cdot \\frac{N^2 d}{B_r} + Nd = O\\left(Nd + \\frac{N^2 d}{B_r}\\right)$\nFrom the SRAM constraint $B_r \\cdot d + B_c \\cdot d + B_r \\cdot B_c \\leq M$, we have $B_r = O(M/d)$. Substituting:\n$$\\text{Flash Attention IO} = O\\left(Nd + \\frac{N^2 d}{M/d}\\right) = O\\left(Nd + \\frac{N^2 d^2}{M}\\right)$$\nFor long sequences where $N^2 d^2 / M \\gg Nd$, this simplifies to $O(N^2 d^2 / M)$ — a factor of $M/d^2$ improvement over standard attention\u0026rsquo;s $O(N^2)$ term.\nMemory: Standard attention requires $O(N^2)$ memory to store $S$ and $P$. Flash Attention avoids materializing these matrices, requiring only $O(Nd)$ for inputs/outputs — enabling much longer sequences.\nReferences # Dao, T., Fu, D. Y., Ermon, S., Rudra, A., \u0026amp; Ré, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. NeurIPS 2022. https://arxiv.org/abs/2205.14135 AWS Neuron Documentation. Neuron Kernel Interface (NKI) Programming Guide. https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/index.html "},{"id":19,"href":"/docs/section-1/index.html","title":" Foundations of Generative Inference","section":"Docs","content":" Foundations of Generative Inference # Importance of software-hardware co-design # As machine learning models and computing systems grow larger and more complex, system performance increasingly depends on how well algorithms and hardware are designed to work together. This principle, known as software–hardware co-design, involves shaping software to exploit the strengths of the hardware (such as parallel processors, memory layout, and precision formats), while simultaneously developing hardware that anticipates the evolving needs of future algorithms.\nHardware vendors commit to a design today while algorithms continue to evolve. If hardware is misaligned with those future workloads, even sophisticated algorithms may run inefficiently, and powerful chips can remain underutilized. Conversely, a mathematically elegant algorithm that fails to map efficiently onto hardware—due to communication overheads, memory bottlenecks, or interconnect latency, will rarely achieve its theoretical potential.\nCo-design bridges this gap by aligning the algorithm design with the physical realities of computation. It encourages optimization of data movement, matching of precision formats to arithmetic units, and rethinking of architectures to minimize communication costs. Modern systems achieve the greatest efficiency when models, compilers, and hardware are tuned together, allowing computation to flow smoothly through every layer. This shift from isolated component optimization to holistic system-level thinking has fueled breakthroughs such as specialized AI accelerators and optimized training and inference pipelines—delivering substantial gains in speed, energy efficiency, and scalability.\n"},{"id":20,"href":"/docs/section-2/index.html","title":"Algorithmic and Modeling-level Inference Optimization","section":"Docs","content":" Algorithmic and Modeling-level Inference Optimization # Overview of Section 2.\n"},{"id":21,"href":"/docs/section-3/index.html","title":"Systems-Level Inference Optimization","section":"Docs","content":" Systems-Level Inference Optimization # Overview of Section 3.\n"},{"id":22,"href":"/docs/section-4/index.html","title":"Open-Source Tools, Frameworks, and Deployment Scenarios","section":"Docs","content":" Open-Source Tools, Frameworks, and Deployment Scenarios # This section covers practical tools and frameworks for efficient GenAI inference, including:\nvLLM: High-throughput LLM serving framework TensorRT: Hardware-optimized compilation Deployment scenarios: Real-world implementation considerations Overview of Section 3.\n"}]