<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="
  Measuring Inference Efficiency
  
  #
  





  


Evaluating the performance of an LLM during inference requires a clear understanding of both speed and cost efficiency.
While model quality and reasoning capability are crucial (and invariably non-negotiable), the practical utility of an LLM, especially in production systems, often hinges on how quickly and economically it can generate responses. To quantify this, several key metrics are used to capture different aspects of inference behavior.
These are visualized and described in details below.">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="https://neuron-science.github.io/docs/section-1/subsection-5/">
  <meta property="og:site_name" content="Inference Optimization">
  <meta property="og:title" content="1.5 Measuring Inference Efficiency">
  <meta property="og:description" content="Measuring Inference Efficiency # Evaluating the performance of an LLM during inference requires a clear understanding of both speed and cost efficiency. While model quality and reasoning capability are crucial (and invariably non-negotiable), the practical utility of an LLM, especially in production systems, often hinges on how quickly and economically it can generate responses. To quantify this, several key metrics are used to capture different aspects of inference behavior. These are visualized and described in details below.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="website">
<title>1.5 Measuring Inference Efficiency | Inference Optimization</title>
<link rel="icon" href="../../../favicon.png" >
<link rel="manifest" href="../../../manifest.json">
<link rel="canonical" href="https://neuron-science.github.io/docs/section-1/subsection-5/">
<link rel="stylesheet" href="../../../book.min.27cffe658670f57112663ed746a421db277c2dd6549965c27f9ad0103cccd990.css" integrity="sha256-J8/&#43;ZYZw9XESZj7XRqQh2yd8LdZUmWXCf5rQEDzM2ZA=" crossorigin="anonymous">
  <script defer src="../../../fuse.min.js"></script>
  <script defer src="../../../en.search.min.9d1d95b9e653c7bee1bc2763af794a73ca266784f4acee3a0d61aaa10baa6eae.js" integrity="sha256-nR2VueZTx77hvCdjr3lKc8omZ4T0rO46DWGqoQuqbq4=" crossorigin="anonymous"></script>
<link rel="alternate" type="application/rss+xml" href="https://neuron-science.github.io/docs/section-1/subsection-5/index.xml" title="Inference Optimization" />
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr" class="book-kind-section book-type-docs book-layout-">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    
<aside class="book-menu">
  <div class="book-menu-content">
    
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="../../../"><span>Inference Optimization</span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>















  
  <ul>
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-1/" class=""> Foundations of Generative Inference</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-1/subsection-1/" class="">1.1 Overview of Transformer Architecture</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-1/subsection-2/" class="">1.2 Llama family of models</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-1/subsection-3/" class="">1.3 Hardware Accelerators for Deep Learning</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-1/subsection-4/" class="">1.4 Roofline Analysis</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-1/subsection-5/" class="active">1.5 Measuring Inference Efficiency</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-2/" class="">Algorithmic and Modeling-level Inference Optimization</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-2/subsection-1/" class="">2.1 Efficient Attention Variants</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-2/subsection-2/" class="">2.2 Mixture of Experts</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-2/subsection-3/" class="">2.3 Model Quantization</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-2/subsection-4/" class="">2.4 Key-Value Caching</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-2/subsection-5/" class="">2.5 Speculative Decoding</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-2/subsection-6/" class="">2.6 Knowledge Distillation</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-3/" class="">Systems-Level Inference Optimization</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-3/subsection-1/" class="">3.1 Paged Attention</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-3/subsection-2/" class="">3.2 Continuous Batching</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-3/subsection-3/" class="">3.3 Chunked Prefill</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-3/subsection-4/" class="">3.4 Disaggregated Inference</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-3/subsection-5/" class="">3.5 Multi-LoRA serving</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-3/subsection-6/" class="">3.6 Compute Graph Optimization</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-3/subsection-7/" class="">3.7 Kernel Fusion</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-4/" class="">Open-Source Tools, Frameworks, and Deployment Scenarios</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-4/subsection-1/" class="">Section 4.1</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>














</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>



  </div>
</aside>
 

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="../../../svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>1.5 Measuring Inference Efficiency</h3>

  <label for="toc-control">
    
    <img src="../../../svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#latency-time-to-first-token">Latency (Time-to-First-Token)</a></li>
    <li><a href="#output-speed-tokens-per-second">Output Speed (Tokens per Second)</a></li>
    <li><a href="#throughput">Throughput</a></li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="measuring-inference-efficiency">
  Measuring Inference Efficiency
  
  <a class="anchor" href="#measuring-inference-efficiency">#</a>
  
</h1>

<link rel="stylesheet" href="../../../katex/katex.min.css" />
<script defer src="../../../katex/katex.min.js"></script>

  <script defer src="../../../katex/auto-render.min.js" onload="renderMathInElement(document.body, {
  &#34;delimiters&#34;: [
    {&#34;left&#34;: &#34;$$&#34;, &#34;right&#34;: &#34;$$&#34;, &#34;display&#34;: true},
    {&#34;left&#34;: &#34;$&#34;, &#34;right&#34;: &#34;$&#34;, &#34;display&#34;: false},
    {&#34;left&#34;: &#34;\\(&#34;, &#34;right&#34;: &#34;\\)&#34;, &#34;display&#34;: false},
    {&#34;left&#34;: &#34;\\[&#34;, &#34;right&#34;: &#34;\\]&#34;, &#34;display&#34;: true}
  ]
}
);"></script>


<p>Evaluating the performance of an LLM during inference requires a clear understanding of both <em>speed</em> and <em>cost</em> efficiency.
While model quality and reasoning capability are crucial (and invariably non-negotiable), the practical utility of an LLM, especially in production systems, often hinges on how quickly and economically it can generate responses. To quantify this, several key metrics are used to capture different aspects of inference behavior.
These are visualized and described in details below.</p>
<div style="text-align: center; margin: 2rem 0;">
  <img src="../../../images/measuring_inference_efficiency/ttft_itl.png" 
       alt="Sliding Window Attention Mechanism" 
       style="width: auto; height: auto; object-fit: contain;" />
  <p style="font-size: 0.9em; color: #666; margin-top: 0.4rem;">
    Visualization of performance metrics TTFT and ITL
  </p>
</div>
<h2 id="latency-time-to-first-token">
  Latency (Time-to-First-Token)
  
  <a class="anchor" href="#latency-time-to-first-token">#</a>
  
</h2>
<p>Time-to-first-token (TTFT) measures the elapsed tie between submitting a prompt to an LLM and receiving the first generated token in response.
It captures the model’s initial latency, encompassing tokenization, prompt encoding, etc., and the first forward pass through the network.
TTFT is especially important for interactive or streaming applications such as chatbots or autocomplete systems, where perceived responsiveness directly affects user experience.</p>
<p>For example, consider the prompt: &ldquo;<em>The quick brown fox jumps over the</em>&rdquo;.
In a next-word-prediction task, the model might output the next token &ldquo;<em>lazy</em>&rdquo;.
If it takes 480 ms from sending the prompt to receiving &ldquo;<em>lazy</em>&rdquo;, then the TTFT is 480 ms, regardless of how quickly subsequent tokens (e.g., &ldquo;<em>dog</em>&rdquo;, &ldquo;<em>.</em>&rdquo;) are generated afterward.
TTFT is a crucial indicator of responsiveness, especially for interactive or low-latency applications, where users perceive delay before the model begins to <em>speak</em>.</p>
<p>Because the prefill stage processes the entire input sequence in a single pass before any decoding begins, TTFT is generally <strong>compute-bound</strong>, i.e., its latency is dominated by dense matrix multiplications and attention operations over long input sequences.</p>
<h2 id="output-speed-tokens-per-second">
  Output Speed (Tokens per Second)
  
  <a class="anchor" href="#output-speed-tokens-per-second">#</a>
  
</h2>
<p>Output tokens per second (OTPS) quantifies how quickly an LLM generates tokens after the first token has appeared.
It is typically measured as the average number of tokens produced per second (tokens/sec) during the streaming phase of inference.
High output speed is crucial for long-form generation and large-scale deployments, where total inference time and serving cost scale with the number of generated tokens.
OTPS can also be measured by its inverse, <strong>inter-token latency (ITL)</strong>.</p>
<p>For example, continuing from the previous prompt, &ldquo;<em>The quick brown fox jumps over the</em>&rdquo;.
If the model generates the next 6 tokens (&quot;<em>lazy dog and runs away.</em>&quot;) in 0.3 seconds, the output speed is 20 tokens/sec.
This indicates that, after the initial TTFT, the model sustains a generation rate of 20 tokens per second until completion.</p>
<p>Unlike prefill, decoding is incremental, i.e., each step depends on the previously generated token, and is therefore often <strong>bandwidth-bound</strong> rather than compute-bound.
The need to repeatedly access and update large key–value (KV) caches for attention makes memory throughput and cache locality key determinants of OTPS.
High OTPS is especially important for long-form generation and large-scale serving workloads, where sustained decoding performance dictates overall system efficiency.</p>
<h2 id="throughput">
  Throughput
  
  <a class="anchor" href="#throughput">#</a>
  
</h2>
<p>While Time-to-First-Token (TTFT) and Output Tokens per Second (OTPS) describe the latency and generation speed of a single inference request, real-world LLM deployments rarely serve one user at a time.
Production systems such as chat services, retrieval-augmented APIs, or model endpoints handle many concurrent prompts arriving continuously, each with varying sequence lengths and response demands.
In such scenarios, performance depends not only on how fast one request completes, but on how efficiently the entire system processes multiple requests simultaneously while utilizing available compute resources.</p>
<p><strong>Throughput</strong> captures this aggregate behavior by measuring the total number of tokens generated per second across all concurrent inferences.
It reflects how effectively the model server converts hardware capacity into useful output, and is often reported as <em>tokens per second per device</em> or <em>tokens per second per deployment</em>. High throughput indicates strong hardware utilization and scheduling efficiency, typically achieved through techniques such as dynamic batching, asynchronous queuing, and pipeline or tensor parallelism.</p>
<p>In deployment benchmarks (e.g., vLLM, TensorRT-LLM), throughput is often measured over the total wall-clock time, which does include TTFT periods across concurrent requests.
It is given by</p>
<p>$$\text{Throughput}_{\text{system}} = \frac{\text{Total tokens generated across all requests}}{\text{Total wall-clock duration}}$$</p>
<p>Throughput differs from TTFT and OTPS in scope and interpretation.
TTFT measures responsiveness—how quickly the first token is produced—while OTPS measures streaming speed once generation begins.
Throughput, in contrast, represents system-level efficiency under concurrency. Although TTFT is not directly included in throughput calculations, long TTFTs can still reduce throughput by delaying when generation can begin.
Systems that overlap the prefill and decoding phases, such as those employing continuous batching (e.g., vLLM) maintain high throughput even when TTFT varies across requests.</p>
<p>For example, suppose a model server processes 16 simultaneous prompts, each generating 100 tokens over 4 seconds.
The system outputs a total of 1,600 tokens, giving a throughput of 400 tokens/sec.
Even if each request has a TTFT of 600 ms, the aggregate throughput remains high because token generation for later requests overlaps with the TTFT of earlier ones.
This illustrates how TTFT governs perceived user latency, while throughput governs aggregate system efficiency and cost in large-scale LLM serving.</p>
<h1 id="references">
  References
  
  <a class="anchor" href="#references">#</a>
  
</h1>
<ol>
<li>Artificial Analysis: Understand the AI landscape to choose the best model and provider for your use case <a href="">https://artificialanalysis.ai/</a></li>
</ol>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>





  
  
  
  <div class="flex flex-wrap justify-between">
    <span>
    
      <a href="../../../docs/section-1/subsection-4/" class="flex align-center book-icon">
        <img src="../../../svg/backward.svg" class="book-icon" alt="Previous" title="1.4 Roofline Analysis" />
        <span>1.4 Roofline Analysis</span>
      </a>
    
    </span>
    <span>
    
      <a href="../../../docs/section-2/" class="flex align-center book-icon">
        <span>Algorithmic and Modeling-level Inference Optimization</span>
        <img src="../../../svg/forward.svg" class="book-icon" alt="Next" title="Algorithmic and Modeling-level Inference Optimization" />
      </a>
    
    </span>
  </div>
  




  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 
      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    

<aside class="book-toc">
  <div class="book-toc-content">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#latency-time-to-first-token">Latency (Time-to-First-Token)</a></li>
    <li><a href="#output-speed-tokens-per-second">Output Speed (Tokens per Second)</a></li>
    <li><a href="#throughput">Throughput</a></li>
  </ul>
</nav>



  </div>
</aside>

 
  </main>

  
</body>
</html>
















