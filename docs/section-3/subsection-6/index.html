<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="
  3.6 Compute Graph Optimization
  
  #
  


  Piecewise CUDA Graph
  
  #
  

Efficiently handling varying input token counts and context lengths across execution steps presents a significant challenge in LLM inference. To address this, vLLM leverages torch.compile to capture CUDA Graphs across multiple bucket sizes, enabling optimized execution paths for different input configurations.
This graph capture mechanism proves highly effective for token-wise operators—operations that process each token independently, such as RoPE and RMSNorm. These operators exhibit predictable computational patterns that can be captured and replayed efficiently through CUDA Graphs.">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="https://neuron-science.github.io/docs/section-3/subsection-6/">
  <meta property="og:site_name" content="Inference Optimization">
  <meta property="og:title" content="3.6 Compute Graph Optimization">
  <meta property="og:description" content="3.6 Compute Graph Optimization # Piecewise CUDA Graph # Efficiently handling varying input token counts and context lengths across execution steps presents a significant challenge in LLM inference. To address this, vLLM leverages torch.compile to capture CUDA Graphs across multiple bucket sizes, enabling optimized execution paths for different input configurations.
This graph capture mechanism proves highly effective for token-wise operators—operations that process each token independently, such as RoPE and RMSNorm. These operators exhibit predictable computational patterns that can be captured and replayed efficiently through CUDA Graphs.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="website">
<title>3.6 Compute Graph Optimization | Inference Optimization</title>
<link rel="icon" href="../../../favicon.png" >
<link rel="manifest" href="../../../manifest.json">
<link rel="canonical" href="https://neuron-science.github.io/docs/section-3/subsection-6/">
<link rel="stylesheet" href="../../../book.min.27cffe658670f57112663ed746a421db277c2dd6549965c27f9ad0103cccd990.css" integrity="sha256-J8/&#43;ZYZw9XESZj7XRqQh2yd8LdZUmWXCf5rQEDzM2ZA=" crossorigin="anonymous">
  <script defer src="../../../fuse.min.js"></script>
  <script defer src="../../../en.search.min.9d1d95b9e653c7bee1bc2763af794a73ca266784f4acee3a0d61aaa10baa6eae.js" integrity="sha256-nR2VueZTx77hvCdjr3lKc8omZ4T0rO46DWGqoQuqbq4=" crossorigin="anonymous"></script>
<link rel="alternate" type="application/rss+xml" href="https://neuron-science.github.io/docs/section-3/subsection-6/index.xml" title="Inference Optimization" />
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr" class="book-kind-section book-type-docs book-layout-">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    
<aside class="book-menu">
  <div class="book-menu-content">
    
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="../../../"><span>Inference Optimization</span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>















  
  <ul>
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-1/" class=""> Foundations of Generative Inference</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-1/subsection-1/" class="">1.1 Overview of Transformer Architecture</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-1/subsection-2/" class="">1.2 Llama family of models</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-1/subsection-3/" class="">1.3 Hardware Accelerators for Deep Learning</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-1/subsection-4/" class="">1.4 Roofline Analysis</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-1/subsection-5/" class="">1.5 Measuring Inference Efficiency</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-2/" class="">Algorithmic and Modeling-level Inference Optimization</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-2/subsection-1/" class="">2.1 Efficient Attention Variants</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-2/subsection-2/" class="">2.2 Mixture of Experts</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-2/subsection-3/" class="">2.3 Model Quantization</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-2/subsection-4/" class="">2.4 Key-Value Caching</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-2/subsection-5/" class="">2.5 Speculative Decoding</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-2/subsection-6/" class="">2.6 Knowledge Distillation</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-3/" class="">Systems-Level Inference Optimization</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-3/subsection-1/" class="">3.1 Paged Attention</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-3/subsection-2/" class="">3.2 Continuous Batching</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-3/subsection-3/" class="">3.3 Chunked Prefill</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-3/subsection-4/" class="">3.4 Disaggregated Inference</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-3/subsection-5/" class="">3.5 Multi-LoRA serving</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-3/subsection-6/" class="active">3.6 Compute Graph Optimization</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-3/subsection-7/" class="">3.7 Kernel Fusion</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-4/" class="">Open-Source Tools, Frameworks, and Deployment Scenarios</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="../../../docs/section-4/subsection-1/" class="">Section 4.1</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>














</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>



  </div>
</aside>
 

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="../../../svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>3.6 Compute Graph Optimization</h3>

  <label for="toc-control">
    
    <img src="../../../svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#piecewise-cuda-graph">Piecewise CUDA Graph</a></li>
    <li><a href="#persistent-kernel">Persistent Kernel</a></li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="36-compute-graph-optimization">
  3.6 Compute Graph Optimization
  
  <a class="anchor" href="#36-compute-graph-optimization">#</a>
  
</h1>
<h2 id="piecewise-cuda-graph">
  Piecewise CUDA Graph
  
  <a class="anchor" href="#piecewise-cuda-graph">#</a>
  
</h2>
<p>Efficiently handling varying input token counts and context lengths across execution steps presents a significant challenge in LLM inference. To address this, vLLM leverages torch.compile to capture CUDA Graphs across multiple bucket sizes, enabling optimized execution paths for different input configurations.</p>
<p>This graph capture mechanism proves highly effective for token-wise operators—operations that process each token independently, such as RoPE and RMSNorm. These operators exhibit predictable computational patterns that can be captured and replayed efficiently through CUDA Graphs.</p>
<p>However, cross-token operators present a different challenge. The attention module, which computes relationships between tokens, must execute in eager mode because it depends on multi-dimensional input shapes that vary dynamically. Exhaustively enumerating all possible shape combinations during compilation would be impractical, making CUDA Graph capture infeasible for these operators.</p>
<p>The following diagram illustrates the Piecewise CUDA Graph architecture in vLLM:</p>
<div style="text-align: center; margin: 2rem 0;">
  <img src="../../../images/graph_optimization/piecewise_cudagraph.png" 
       alt="Piecewise CUDA Graph in vLLM" 
       style="width: auto; height: auto; object-fit: contain;" />
  <p style="font-size: 0.9em; color: #666; margin-top: 0.4rem;">
    Token-wise operators like QKV projection and MLP layers are captured with torch.compile for CUDA Graph execution (light green). Cross-token operators like the attention module execute in eager mode (light yellow). (credits: 8th vLLM Meetup Slides)
  </p>
</div>
<p>By separating token-wise and cross-token operators, the piecewise CUDA Graph approach achieves high-performance execution through CUDA Graphs where possible, while maintaining the flexibility to handle dynamic shapes in attention operations. This hybrid execution model delivers the performance benefits of graph optimization without imposing restrictive shape constraints on the system.</p>
<h2 id="persistent-kernel">
  Persistent Kernel
  
  <a class="anchor" href="#persistent-kernel">#</a>
  
</h2>
<p>While CUDA Graphs reduce both CPU overhead and kernel launch latency to some extent, persistent kernels take optimization further by producing megakernels—large, unified kernels that eliminate kernel launch overhead entirely and remove pipeline bubbles between operations.</p>
<p>The Mirage Persistent Kernel (MPK) framework implements this approach through a sophisticated two-stage process. First, the MPK compiler analyzes the computation graph and decomposes it into a fine-grained task graph, where each task represents a unit of work with explicit dependencies on other tasks. This task-level representation enables more flexible scheduling and resource utilization than traditional kernel-by-kernel execution.</p>
<p>Second, the MPK runtime orchestrates execution through a worker-scheduler architecture. Worker threads actively poll the task queue and execute tasks as they become available. Meanwhile, scheduler threads monitor task dependencies and emit activation events when all prerequisites for a task are satisfied. This event-driven coordination ensures tasks execute as soon as their dependencies are met, maximizing hardware utilization and minimizing idle time.</p>
<div style="text-align: center; margin: 2rem 0;">
  <img src="../../../images/graph_optimization/mirage.png" 
       alt="The MPK runtime executes a task graph in a megakernel" 
       style="width: auto; height: auto; object-fit: contain;" />
  <p style="font-size: 0.9em; color: #666; margin-top: 0.4rem;">
    The MPK runtime executes a task graph within a single megakernel, with workers processing tasks and schedulers managing dependencies. (credits: Compiling LLMs into a MegaKernel: A Path to Low-Latency Inference, Zhihao Jia)
  </p>
</div>
<p>By consolidating multiple operations into a single persistent megakernel, MPK eliminates the overhead of launching individual kernels and the synchronization costs between them, resulting in substantially reduced inference latency for LLM workloads.</p>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>





  
  
  
  <div class="flex flex-wrap justify-between">
    <span>
    
      <a href="../../../docs/section-3/subsection-5/" class="flex align-center book-icon">
        <img src="../../../svg/backward.svg" class="book-icon" alt="Previous" title="3.5 Multi-LoRA serving" />
        <span>3.5 Multi-LoRA serving</span>
      </a>
    
    </span>
    <span>
    
      <a href="../../../docs/section-3/subsection-7/" class="flex align-center book-icon">
        <span>3.7 Kernel Fusion</span>
        <img src="../../../svg/forward.svg" class="book-icon" alt="Next" title="3.7 Kernel Fusion" />
      </a>
    
    </span>
  </div>
  




  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 
      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    

<aside class="book-toc">
  <div class="book-toc-content">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#piecewise-cuda-graph">Piecewise CUDA Graph</a></li>
    <li><a href="#persistent-kernel">Persistent Kernel</a></li>
  </ul>
</nav>



  </div>
</aside>

 
  </main>

  
</body>
</html>
















